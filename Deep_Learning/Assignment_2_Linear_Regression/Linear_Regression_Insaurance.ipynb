{"cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "id": "TcrrErJaRB8v"}, "source": ["# Insurance cost prediction using linear regression\n", "\n", "Make a submisson here: https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans/assignment/assignment-2-train-your-first-model\n", "\n", "In this assignment we're going to use information like a person's age, sex, BMI, no. of children and smoking habit to predict the price of yearly medical bills. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from [Kaggle](https://www.kaggle.com/mirichoi0218/insurance).\n", "\n", "\n", "We will create a model with the following steps:\n", "1. Download and explore the dataset\n", "2. Prepare the dataset for training\n", "3. Create a linear regression model\n", "4. Train the model to fit the data\n", "5. Make predictions using the trained model\n", "\n", "\n", "This assignment builds upon the concepts from the first 2 lessons. It will help to review these Jupyter notebooks:\n", "- PyTorch basics: https://jovian.ai/aakashns/01-pytorch-basics\n", "- Linear Regression: https://jovian.ai/aakashns/02-linear-regression\n", "- Logistic Regression: https://jovian.ai/aakashns/03-logistic-regression\n", "- Linear regression (minimal): https://jovian.ai/aakashns/housing-linear-minimal\n", "- Logistic regression (minimal): https://jovian.ai/aakashns/mnist-logistic-minimal\n", "\n", "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end . In some cases, you'll be required to choose some hyperparameters (learning rate, batch size etc.). Try to experiment with the hypeparameters to get the lowest loss.\n"]}, {"cell_type": "code", "execution_count": 134, "metadata": {"executionInfo": {"elapsed": 1023, "status": "ok", "timestamp": 1607753141812, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "3b-xxXRlRB8w"}, "outputs": [], "source": ["# Uncomment and run the appropriate command for your operating system, if required\n", "\n", "# Linux / Binder\n", "# !pip install numpy matplotlib pandas torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n", "\n", "# Windows\n", "# !pip install numpy matplotlib pandas torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n", "\n", "# MacOS\n", "# !pip install numpy matplotlib pandas torch torchvision torchaudio"]}, {"cell_type": "code", "execution_count": 135, "metadata": {"executionInfo": {"elapsed": 1203, "status": "ok", "timestamp": 1607753142361, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "u4tZNIv3RB8x"}, "outputs": [], "source": ["import torch\n", "import jovian\n", "import torchvision\n", "import torch.nn as nn\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import torch.nn.functional as F\n", "from torchvision.datasets.utils import download_url\n", "from torch.utils.data import DataLoader, TensorDataset, random_split"]}, {"cell_type": "code", "execution_count": 136, "metadata": {"executionInfo": {"elapsed": 1378, "status": "ok", "timestamp": 1607753143112, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "ioP4WzU1RB8x"}, "outputs": [], "source": ["project_name='02-insurance-linear-regression' # will be used by jovian.commit"]}, {"cell_type": "markdown", "metadata": {"id": "1VbDYGDmRB8y"}, "source": ["## Step 1: Download and explore the data\n", "\n", "Let us begin by downloading the data. We'll use the `download_url` function from PyTorch to get the data as a CSV (comma-separated values) file. "]}, {"cell_type": "code", "execution_count": 137, "metadata": {"_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0", "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 863, "status": "ok", "timestamp": 1607753143811, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "ai6hXHgHRB8y", "outputId": "3733d289-c079-4394-a2c4-3a80bbe352ae"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Using downloaded and verified file: ./insurance.csv\n"]}], "source": ["DATASET_URL = \"https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv\"\n", "DATA_FILENAME = \"insurance.csv\"\n", "download_url(DATASET_URL, '.')"]}, {"cell_type": "markdown", "metadata": {"id": "dsccew3sRB80"}, "source": ["To load the dataset into memory, we'll use the `read_csv` function from the `pandas` library. The data will be loaded as a Pandas dataframe. See this short tutorial to learn more: https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/"]}, {"cell_type": "code", "execution_count": 138, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 204}, "executionInfo": {"elapsed": 1061, "status": "ok", "timestamp": 1607753145569, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "Wcjh1876RB80", "outputId": "068284e6-981e-4c82-c268-0aae370e5247"}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>age</th>\n", "      <th>sex</th>\n", "      <th>bmi</th>\n", "      <th>children</th>\n", "      <th>smoker</th>\n", "      <th>region</th>\n", "      <th>charges</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>19</td>\n", "      <td>female</td>\n", "      <td>27.900</td>\n", "      <td>0</td>\n", "      <td>yes</td>\n", "      <td>southwest</td>\n", "      <td>16884.92400</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>18</td>\n", "      <td>male</td>\n", "      <td>33.770</td>\n", "      <td>1</td>\n", "      <td>no</td>\n", "      <td>southeast</td>\n", "      <td>1725.55230</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>28</td>\n", "      <td>male</td>\n", "      <td>33.000</td>\n", "      <td>3</td>\n", "      <td>no</td>\n", "      <td>southeast</td>\n", "      <td>4449.46200</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>33</td>\n", "      <td>male</td>\n", "      <td>22.705</td>\n", "      <td>0</td>\n", "      <td>no</td>\n", "      <td>northwest</td>\n", "      <td>21984.47061</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>32</td>\n", "      <td>male</td>\n", "      <td>28.880</td>\n", "      <td>0</td>\n", "      <td>no</td>\n", "      <td>northwest</td>\n", "      <td>3866.85520</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   age     sex     bmi  children smoker     region      charges\n", "0   19  female  27.900         0    yes  southwest  16884.92400\n", "1   18    male  33.770         1     no  southeast   1725.55230\n", "2   28    male  33.000         3     no  southeast   4449.46200\n", "3   33    male  22.705         0     no  northwest  21984.47061\n", "4   32    male  28.880         0     no  northwest   3866.85520"]}, "execution_count": 138, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["dataframe_raw = pd.read_csv(DATA_FILENAME)\n", "dataframe_raw.head()"]}, {"cell_type": "markdown", "metadata": {"id": "Vb3_vSmGRB81"}, "source": ["We're going to do a slight customization of the data, so that you every participant receives a slightly different version of the dataset. Fill in your name below as a string (enter at least 5 characters)"]}, {"cell_type": "code", "execution_count": 139, "metadata": {"executionInfo": {"elapsed": 1595, "status": "ok", "timestamp": 1607753147480, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "9wsLKpdkRB81"}, "outputs": [], "source": ["your_name = 'Anurag Kumar Singh' # at least 5 characters"]}, {"cell_type": "markdown", "metadata": {"id": "nUeaxzoGRB82"}, "source": ["The `customize_dataset` function will customize the dataset slightly using your name as a source of random numbers."]}, {"cell_type": "code", "execution_count": 140, "metadata": {"executionInfo": {"elapsed": 1419, "status": "ok", "timestamp": 1607753148203, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "ewOTcJ4BRB82"}, "outputs": [], "source": ["def customize_dataset(dataframe_raw, rand_str):\n", "    dataframe = dataframe_raw.copy(deep=True)\n", "    # drop some rows\n", "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n", "    # scale input\n", "    dataframe.bmi = dataframe.bmi * ord(rand_str[1])/100.\n", "    # scale target\n", "    dataframe.charges = dataframe.charges * ord(rand_str[2])/100.\n", "    # drop column\n", "    if ord(rand_str[3]) % 2 == 1:\n", "        dataframe = dataframe.drop(['region'], axis=1)\n", "    return dataframe"]}, {"cell_type": "code", "execution_count": 141, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 204}, "executionInfo": {"elapsed": 1068, "status": "ok", "timestamp": 1607753148205, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "gwuaRFrLRB82", "outputId": "ae5e6129-0d5c-4d48-e6be-8ad193e5a752"}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>age</th>\n", "      <th>sex</th>\n", "      <th>bmi</th>\n", "      <th>children</th>\n", "      <th>smoker</th>\n", "      <th>region</th>\n", "      <th>charges</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>27</th>\n", "      <td>55</td>\n", "      <td>female</td>\n", "      <td>36.0525</td>\n", "      <td>2</td>\n", "      <td>no</td>\n", "      <td>northwest</td>\n", "      <td>14354.299732</td>\n", "    </tr>\n", "    <tr>\n", "      <th>752</th>\n", "      <td>64</td>\n", "      <td>male</td>\n", "      <td>41.6955</td>\n", "      <td>0</td>\n", "      <td>no</td>\n", "      <td>northwest</td>\n", "      <td>16626.327062</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1258</th>\n", "      <td>55</td>\n", "      <td>male</td>\n", "      <td>41.4865</td>\n", "      <td>3</td>\n", "      <td>no</td>\n", "      <td>northwest</td>\n", "      <td>35174.389244</td>\n", "    </tr>\n", "    <tr>\n", "      <th>384</th>\n", "      <td>44</td>\n", "      <td>male</td>\n", "      <td>24.3485</td>\n", "      <td>2</td>\n", "      <td>no</td>\n", "      <td>northeast</td>\n", "      <td>9713.966711</td>\n", "    </tr>\n", "    <tr>\n", "      <th>406</th>\n", "      <td>33</td>\n", "      <td>female</td>\n", "      <td>26.7410</td>\n", "      <td>0</td>\n", "      <td>no</td>\n", "      <td>southeast</td>\n", "      <td>4896.564543</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["      age     sex      bmi  children smoker     region       charges\n", "27     55  female  36.0525         2     no  northwest  14354.299732\n", "752    64    male  41.6955         0     no  northwest  16626.327062\n", "1258   55    male  41.4865         3     no  northwest  35174.389244\n", "384    44    male  24.3485         2     no  northeast   9713.966711\n", "406    33  female  26.7410         0     no  southeast   4896.564543"]}, "execution_count": 141, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["dataframe = customize_dataset(dataframe_raw, your_name)\n", "dataframe.head()"]}, {"cell_type": "markdown", "metadata": {"id": "hU-7nEHiRB83"}, "source": ["Let us answer some basic questions about the dataset. \n", "\n", "\n", "**Q: How many rows does the dataset have?**"]}, {"cell_type": "code", "execution_count": 142, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1083, "status": "ok", "timestamp": 1607753149170, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "P0_5TMMBRB83", "outputId": "475b6021-c562-4c12-b733-ccff0e037b8c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1271\n"]}], "source": ["num_rows = dataframe.shape[0]\n", "print(num_rows)"]}, {"cell_type": "markdown", "metadata": {"id": "v5cvScAFRB83"}, "source": ["**Q: How many columns doe the dataset have**"]}, {"cell_type": "code", "execution_count": 143, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 902, "status": "ok", "timestamp": 1607753149753, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "qa0n8UneRB83", "outputId": "b8ac0f89-5ad4-42e5-cf4b-629d84ef4832"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["7\n"]}], "source": ["num_cols = dataframe.shape[1]\n", "print(num_cols)"]}, {"cell_type": "markdown", "metadata": {"id": "srGyoZXCRB84"}, "source": ["**Q: What are the column titles of the input variables?**"]}, {"cell_type": "code", "execution_count": 144, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1077, "status": "ok", "timestamp": 1607753150739, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "mtzsPUcwRB84", "outputId": "579bb1ca-eade-4ea8-9bed-7684f03ffd8a"}, "outputs": [{"data": {"text/plain": ["Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region'], dtype='object')"]}, "execution_count": 144, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["input_cols = dataframe.columns[0:-1]\n", "input_cols"]}, {"cell_type": "markdown", "metadata": {"id": "UN47XP9ORB84"}, "source": ["**Q: Which of the input columns are non-numeric or categorial variables ?**\n", "\n", "Hint: `sex` is one of them. List the columns that are not numbers."]}, {"cell_type": "code", "execution_count": 145, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1585, "status": "ok", "timestamp": 1607753151993, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "XDOfhiMZRB84", "outputId": "9738d01c-4f74-49ad-83d9-9b02877c960b"}, "outputs": [{"data": {"text/plain": ["['sex', 'smoker', 'region']"]}, "execution_count": 145, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["categorical_cols = []\n", "for i in input_cols:\n", "  if str(dataframe.dtypes[i]) != 'int64' and str(dataframe.dtypes[i]) != 'float64':\n", "    categorical_cols.append(i)\n", "\n", "categorical_cols"]}, {"cell_type": "markdown", "metadata": {"id": "Mu_yC7IfRB85"}, "source": ["**Q: What are the column titles of output/target variable(s)?**"]}, {"cell_type": "code", "execution_count": 146, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1151, "status": "ok", "timestamp": 1607753152471, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "wKxghfHGRB85", "outputId": "9d83da2f-0c9f-44b8-9110-c14776935756"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['charges']\n"]}], "source": ["output_cols = [dataframe.columns[-1]]\n", "print(output_cols)"]}, {"cell_type": "markdown", "metadata": {"id": "60MYxbCfRB85"}, "source": ["**Q: (Optional) What is the minimum, maximum and average value of the `charges` column? Can you show the distribution of values in a graph?**\n", "Use this data visualization cheatsheet for referece: https://jovian.ml/aakashns/dataviz-cheatsheet"]}, {"cell_type": "code", "execution_count": 147, "metadata": {"executionInfo": {"elapsed": 987, "status": "ok", "timestamp": 1607753153607, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "eh7KkghiRB86"}, "outputs": [], "source": ["# Write your answer here"]}, {"cell_type": "markdown", "metadata": {"id": "t6FkIpp6RB86"}, "source": ["Remember to commit your notebook to Jovian after every step, so that you don't lose your work."]}, {"cell_type": "code", "execution_count": 148, "metadata": {"executionInfo": {"elapsed": 4073, "status": "ok", "timestamp": 1607753157559, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "l0Awn9mpRB86"}, "outputs": [], "source": ["!pip install jovian --upgrade -q"]}, {"cell_type": "code", "execution_count": 149, "metadata": {"executionInfo": {"elapsed": 3713, "status": "ok", "timestamp": 1607753157561, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "wybteLGmRB87"}, "outputs": [], "source": ["import jovian"]}, {"cell_type": "code", "execution_count": 150, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 120}, "executionInfo": {"elapsed": 8053, "status": "ok", "timestamp": 1607753162297, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "iexwkaYnRB87", "outputId": "0a07cf88-9a82-43b9-e238-da9acf1a8082"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[jovian] Detected Colab notebook...\u001b[0m\n", "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n", "[jovian] Capturing environment..\u001b[0m\n", "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n", "[jovian] Committed successfully! https://jovian.ai/anurag3301/02-insurance-linear-regression\u001b[0m\n"]}, {"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'https://jovian.ai/anurag3301/02-insurance-linear-regression'"]}, "execution_count": 150, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["jovian.commit(project=project_name)"]}, {"cell_type": "markdown", "metadata": {"id": "1IKl5MlrRB88"}, "source": ["## Step 2: Prepare the dataset for training\n", "\n", "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays."]}, {"cell_type": "code", "execution_count": 151, "metadata": {"executionInfo": {"elapsed": 1130, "status": "ok", "timestamp": 1607753173491, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "XdOf1qO2RB88"}, "outputs": [], "source": ["def dataframe_to_arrays(dataframe):\n", "    # Make a copy of the original dataframe\n", "    dataframe1 = dataframe.copy(deep=True)\n", "    # Convert non-numeric categorical columns to numbers\n", "    for col in categorical_cols:\n", "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n", "    # Extract input & outupts as numpy arrays\n", "    inputs_array = dataframe1[input_cols].to_numpy()\n", "    targets_array = dataframe1[output_cols].to_numpy()\n", "    return inputs_array, targets_array"]}, {"cell_type": "markdown", "metadata": {"id": "OdjrouaGRB8-"}, "source": ["Read through the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) to understand how we're converting categorical variables into numbers."]}, {"cell_type": "code", "execution_count": 152, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1854, "status": "ok", "timestamp": 1607753174236, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "4sLHnK8TRB8-", "outputId": "5434f8c0-9391-47ba-ae9b-dddf284ab600"}, "outputs": [{"data": {"text/plain": ["(array([[55.    ,  0.    , 36.0525,  2.    ,  0.    ,  1.    ],\n", "        [64.    ,  1.    , 41.6955,  0.    ,  0.    ,  1.    ],\n", "        [55.    ,  1.    , 41.4865,  3.    ,  0.    ,  1.    ],\n", "        ...,\n", "        [56.    ,  1.    , 28.5285,  0.    ,  0.    ,  0.    ],\n", "        [18.    ,  0.    , 33.3355,  0.    ,  0.    ,  0.    ],\n", "        [43.    ,  1.    , 28.633 ,  0.    ,  0.    ,  0.    ]]),\n", " array([[14354.2997325],\n", "        [16626.3270615],\n", "        [35174.3892435],\n", "        ...,\n", "        [13063.5386505],\n", "        [ 2578.3710615],\n", "        [ 7999.721379 ]]))"]}, "execution_count": 152, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["inputs_array, targets_array = dataframe_to_arrays(dataframe)\n", "inputs_array, targets_array"]}, {"cell_type": "markdown", "metadata": {"id": "CDxXIOJyRB8-"}, "source": ["**Q: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**"]}, {"cell_type": "code", "execution_count": 153, "metadata": {"executionInfo": {"elapsed": 1848, "status": "ok", "timestamp": 1607753174237, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "yv_O2JHpRB8-"}, "outputs": [], "source": ["inputs = torch.tensor(inputs_array, dtype=torch.float32)\n", "targets = torch.tensor(targets_array, dtype=torch.float32)"]}, {"cell_type": "code", "execution_count": 154, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1838, "status": "ok", "timestamp": 1607753174238, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "3rXYU9rJRB8_", "outputId": "f61f6ed7-599d-464e-9b62-fa500739f075"}, "outputs": [{"data": {"text/plain": ["(torch.float32, torch.float32)"]}, "execution_count": 154, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["inputs.dtype, targets.dtype"]}, {"cell_type": "markdown", "metadata": {"id": "e5jBtYsnRB8_"}, "source": ["Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`."]}, {"cell_type": "code", "execution_count": 155, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1828, "status": "ok", "timestamp": 1607753174239, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "s2quVhGkRB8_", "outputId": "16f4908f-28fe-4050-e85d-68f2dd5587c9"}, "outputs": [{"data": {"text/plain": ["1271"]}, "execution_count": 155, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["dataset = TensorDataset(inputs, targets)\n", "len(dataset)"]}, {"cell_type": "markdown", "metadata": {"id": "yOP_kfwgRB8_"}, "source": ["**Q: Pick a number between `0.1` and `0.2` to determine the fraction of data that will be used for creating the validation set. Then use `random_split` to create training & validation datasets.**"]}, {"cell_type": "code", "execution_count": 156, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1819, "status": "ok", "timestamp": 1607753174241, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "hoFJ05shRB9A", "outputId": "00528796-977c-44fe-f310-3c44f80eb077"}, "outputs": [{"data": {"text/plain": ["(1208, 63)"]}, "execution_count": 156, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["val_percent = 0.05 # between 0.1 and 0.2\n", "val_size = int(num_rows * val_percent)\n", "train_size = num_rows - val_size\n", "\n", "\n", "train_ds, val_ds = random_split(dataset, (train_size, val_size)) # Use the random_split function to split dataset into 2 parts of the desired length\n", "len(train_ds), len(val_ds)"]}, {"cell_type": "markdown", "metadata": {"id": "wwwJMv8tRB9A"}, "source": ["Finally, we can create data loaders for training & validation.\n", "\n", "**Q: Pick a batch size for the data loader.**"]}, {"cell_type": "code", "execution_count": 157, "metadata": {"executionInfo": {"elapsed": 1817, "status": "ok", "timestamp": 1607753174243, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "wes0RuDYRB9A"}, "outputs": [], "source": ["batch_size = 100"]}, {"cell_type": "code", "execution_count": 158, "metadata": {"executionInfo": {"elapsed": 1815, "status": "ok", "timestamp": 1607753174245, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "sJOhHeEcRB9A"}, "outputs": [], "source": ["train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n", "val_loader = DataLoader(val_ds, batch_size)"]}, {"cell_type": "markdown", "metadata": {"id": "guExZZWFRB9A"}, "source": ["Let's look at a batch of data to verify everything is working fine so far."]}, {"cell_type": "code", "execution_count": 159, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1809, "status": "ok", "timestamp": 1607753174247, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "5-ku-F0VRB9B", "outputId": "80fcd2e0-02e6-4d41-f47a-a0dfe711567a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["inputs: tensor([[35.0000,  1.0000, 42.4600,  1.0000,  0.0000,  3.0000],\n", "        [64.0000,  0.0000, 35.0075,  2.0000,  0.0000,  0.0000],\n", "        [54.0000,  0.0000, 23.6170,  3.0000,  0.0000,  1.0000],\n", "        [47.0000,  0.0000, 32.3070,  1.0000,  0.0000,  2.0000],\n", "        [34.0000,  0.0000, 29.0510,  1.0000,  0.0000,  1.0000],\n", "        [58.0000,  1.0000, 39.2700,  0.0000,  0.0000,  3.0000],\n", "        [63.0000,  1.0000, 23.8260,  1.0000,  0.0000,  1.0000],\n", "        [61.0000,  0.0000, 32.9120,  3.0000,  1.0000,  2.0000],\n", "        [59.0000,  0.0000, 30.4920,  3.0000,  0.0000,  2.0000],\n", "        [24.0000,  1.0000, 34.1715,  0.0000,  1.0000,  0.0000],\n", "        [21.0000,  1.0000, 34.1220,  0.0000,  0.0000,  2.0000],\n", "        [46.0000,  0.0000, 35.5300,  2.0000,  0.0000,  0.0000],\n", "        [30.0000,  0.0000, 35.6400,  1.0000,  0.0000,  3.0000],\n", "        [59.0000,  0.0000, 40.4415,  1.0000,  1.0000,  0.0000],\n", "        [38.0000,  0.0000, 29.9915,  1.0000,  0.0000,  0.0000],\n", "        [55.0000,  0.0000, 32.8130,  0.0000,  0.0000,  0.0000],\n", "        [36.0000,  1.0000, 38.7200,  1.0000,  1.0000,  2.0000],\n", "        [48.0000,  1.0000, 35.5300,  1.0000,  0.0000,  1.0000],\n", "        [46.0000,  1.0000, 36.7840,  1.0000,  0.0000,  0.0000],\n", "        [54.0000,  1.0000, 33.0220,  0.0000,  0.0000,  1.0000],\n", "        [52.0000,  0.0000, 20.1685,  0.0000,  0.0000,  1.0000],\n", "        [24.0000,  0.0000, 32.9175,  0.0000,  0.0000,  1.0000],\n", "        [19.0000,  0.0000, 33.6490,  2.0000,  0.0000,  1.0000],\n", "        [54.0000,  1.0000, 33.2310,  0.0000,  0.0000,  1.0000],\n", "        [59.0000,  0.0000, 35.3100,  3.0000,  0.0000,  3.0000],\n", "        [46.0000,  1.0000, 36.6795,  1.0000,  0.0000,  0.0000],\n", "        [53.0000,  0.0000, 30.9100,  3.0000,  0.0000,  3.0000],\n", "        [50.0000,  0.0000, 30.9320,  3.0000,  0.0000,  1.0000],\n", "        [57.0000,  0.0000, 24.4530,  0.0000,  0.0000,  0.0000],\n", "        [20.0000,  1.0000, 38.8410,  1.0000,  0.0000,  2.0000],\n", "        [31.0000,  0.0000, 29.2820,  0.0000,  0.0000,  2.0000],\n", "        [60.0000,  0.0000, 33.5500,  0.0000,  0.0000,  3.0000],\n", "        [29.0000,  1.0000, 34.9030,  2.0000,  0.0000,  1.0000],\n", "        [18.0000,  1.0000, 37.0260,  0.0000,  0.0000,  2.0000],\n", "        [37.0000,  0.0000, 25.7070,  2.0000,  0.0000,  1.0000],\n", "        [38.0000,  0.0000, 44.1650,  0.0000,  0.0000,  2.0000],\n", "        [60.0000,  0.0000, 41.8660,  0.0000,  0.0000,  2.0000],\n", "        [30.0000,  1.0000, 48.6420,  2.0000,  0.0000,  2.0000],\n", "        [58.0000,  1.0000, 25.6300,  0.0000,  0.0000,  3.0000],\n", "        [56.0000,  0.0000, 31.1410,  0.0000,  0.0000,  0.0000],\n", "        [40.0000,  1.0000, 27.5880,  0.0000,  0.0000,  2.0000],\n", "        [41.0000,  0.0000, 35.4200,  1.0000,  0.0000,  3.0000],\n", "        [30.0000,  0.0000, 47.4320,  2.0000,  0.0000,  2.0000],\n", "        [24.0000,  1.0000, 29.4690,  1.0000,  0.0000,  1.0000],\n", "        [25.0000,  0.0000, 25.8115,  0.0000,  0.0000,  0.0000],\n", "        [40.0000,  1.0000, 28.9465,  1.0000,  0.0000,  1.0000],\n", "        [28.0000,  0.0000, 26.2295,  2.0000,  0.0000,  1.0000],\n", "        [30.0000,  1.0000, 26.5430,  1.0000,  0.0000,  1.0000],\n", "        [47.0000,  0.0000, 36.6795,  0.0000,  0.0000,  0.0000],\n", "        [47.0000,  1.0000, 27.9510,  1.0000,  1.0000,  2.0000],\n", "        [57.0000,  0.0000, 32.7910,  0.0000,  1.0000,  2.0000],\n", "        [25.0000,  1.0000, 28.8420,  0.0000,  0.0000,  0.0000],\n", "        [18.0000,  0.0000, 35.1120,  0.0000,  0.0000,  0.0000],\n", "        [35.0000,  0.0000, 47.6740,  2.0000,  0.0000,  2.0000],\n", "        [22.0000,  1.0000, 29.5240,  0.0000,  0.0000,  2.0000],\n", "        [34.0000,  1.0000, 27.7970,  1.0000,  0.0000,  1.0000],\n", "        [64.0000,  0.0000, 43.6700,  0.0000,  0.0000,  3.0000],\n", "        [43.0000,  0.0000, 29.5735,  0.0000,  1.0000,  1.0000],\n", "        [26.0000,  0.0000, 32.4280,  1.0000,  0.0000,  2.0000],\n", "        [31.0000,  1.0000, 33.9625,  0.0000,  0.0000,  0.0000],\n", "        [20.0000,  1.0000, 30.0300,  0.0000,  1.0000,  3.0000],\n", "        [33.0000,  0.0000, 42.7900,  3.0000,  0.0000,  3.0000],\n", "        [60.0000,  1.0000, 31.7900,  0.0000,  0.0000,  3.0000],\n", "        [24.0000,  1.0000, 35.9700,  0.0000,  1.0000,  3.0000],\n", "        [45.0000,  0.0000, 33.5445,  1.0000,  1.0000,  1.0000],\n", "        [19.0000,  1.0000, 37.5100,  0.0000,  0.0000,  3.0000],\n", "        [24.0000,  0.0000, 24.8600,  0.0000,  0.0000,  3.0000],\n", "        [62.0000,  1.0000, 29.3645,  0.0000,  1.0000,  0.0000],\n", "        [21.0000,  1.0000, 28.3195,  2.0000,  0.0000,  0.0000],\n", "        [39.0000,  0.0000, 45.9800,  0.0000,  0.0000,  2.0000],\n", "        [34.0000,  1.0000, 29.7000,  2.0000,  0.0000,  3.0000],\n", "        [59.0000,  1.0000, 28.0060,  1.0000,  0.0000,  0.0000],\n", "        [48.0000,  1.0000, 39.1875,  4.0000,  0.0000,  0.0000],\n", "        [26.0000,  0.0000, 44.2035,  0.0000,  0.0000,  1.0000],\n", "        [36.0000,  1.0000, 37.8730,  2.0000,  0.0000,  2.0000],\n", "        [34.0000,  1.0000, 36.0800,  1.0000,  0.0000,  3.0000],\n", "        [34.0000,  0.0000, 32.1860,  3.0000,  0.0000,  2.0000],\n", "        [47.0000,  0.0000, 40.2930,  1.0000,  1.0000,  2.0000],\n", "        [40.0000,  0.0000, 30.1400,  1.0000,  0.0000,  3.0000],\n", "        [19.0000,  0.0000, 25.7400,  2.0000,  0.0000,  3.0000],\n", "        [18.0000,  1.0000, 28.0060,  0.0000,  0.0000,  0.0000],\n", "        [38.0000,  0.0000, 33.2310,  3.0000,  0.0000,  1.0000],\n", "        [31.0000,  1.0000, 30.4095,  2.0000,  0.0000,  0.0000],\n", "        [31.0000,  1.0000, 22.4400,  0.0000,  0.0000,  3.0000],\n", "        [19.0000,  1.0000, 22.9900,  1.0000,  0.0000,  3.0000],\n", "        [63.0000,  1.0000, 40.4415,  0.0000,  0.0000,  0.0000],\n", "        [50.0000,  1.0000, 49.2470,  1.0000,  0.0000,  2.0000],\n", "        [19.0000,  0.0000, 31.4600,  5.0000,  0.0000,  3.0000],\n", "        [35.0000,  0.0000, 37.5155,  3.0000,  1.0000,  1.0000],\n", "        [36.0000,  1.0000, 46.0845,  3.0000,  1.0000,  0.0000],\n", "        [30.0000,  1.0000, 25.2890,  2.0000,  1.0000,  1.0000],\n", "        [42.0000,  1.0000, 27.0655,  2.0000,  1.0000,  0.0000],\n", "        [28.0000,  0.0000, 38.2470,  0.0000,  0.0000,  1.0000],\n", "        [18.0000,  0.0000, 23.8260,  0.0000,  1.0000,  0.0000],\n", "        [21.0000,  0.0000, 38.3570,  0.0000,  0.0000,  2.0000],\n", "        [18.0000,  0.0000, 35.3320,  2.0000,  0.0000,  2.0000],\n", "        [51.0000,  1.0000, 43.6700,  1.0000,  0.0000,  3.0000],\n", "        [40.0000,  0.0000, 36.0525,  2.0000,  1.0000,  1.0000],\n", "        [36.0000,  1.0000, 33.9625,  1.0000,  0.0000,  1.0000],\n", "        [30.0000,  0.0000, 30.4700,  0.0000,  0.0000,  3.0000]])\n", "targets: tensor([[ 5571.9248],\n", "        [18800.8301],\n", "        [14596.1611],\n", "        [10000.7988],\n", "        [ 6300.8452],\n", "        [13294.4238],\n", "        [16789.3301],\n", "        [36202.3633],\n", "        [16381.3262],\n", "        [40077.2422],\n", "        [19406.2031],\n", "        [11010.8760],\n", "        [ 4855.1909],\n", "        [56039.2461],\n", "        [ 7669.4321],\n", "        [13205.2500],\n", "        [45289.7344],\n", "        [10255.3418],\n", "        [ 9751.4697],\n", "        [28637.4805],\n", "        [11689.5137],\n", "        [ 3335.3000],\n", "        [28149.8262],\n", "        [11970.8545],\n", "        [16388.4492],\n", "        [ 9751.3154],\n", "        [13737.8193],\n", "        [12970.1367],\n", "        [14074.2656],\n", "        [32437.4180],\n", "        [ 4396.6782],\n", "        [14786.6885],\n", "        [ 5187.0635],\n", "        [ 1329.5873],\n", "        [ 7823.1245],\n", "        [ 6319.1470],\n", "        [14798.9834],\n", "        [ 4991.4141],\n", "        [13274.2568],\n", "        [13639.5312],\n", "        [ 6336.3237],\n", "        [ 7927.8745],\n", "        [ 5561.7549],\n", "        [14753.5674],\n", "        [ 3751.5950],\n", "        [ 7475.5723],\n", "        [ 5522.0918],\n", "        [ 4717.7217],\n", "        [24428.1777],\n", "        [25715.0527],\n", "        [32214.6777],\n", "        [ 3183.9453],\n", "        [ 2580.9976],\n", "        [ 6840.8936],\n", "        [ 1948.0496],\n", "        [ 5726.8613],\n", "        [16753.2656],\n", "        [25475.9570],\n", "        [ 3969.0674],\n", "        [ 4513.5781],\n", "        [18992.4316],\n", "        [ 6987.6821],\n", "        [14211.9561],\n", "        [40333.2227],\n", "        [46478.8555],\n", "        [ 1475.8871],\n", "        [ 2875.2773],\n", "        [32878.5586],\n", "        [ 3837.4463],\n", "        [ 6624.8032],\n", "        [13733.2832],\n", "        [15109.3711],\n", "        [12562.1387],\n", "        [ 3745.4568],\n", "        [ 6533.6377],\n", "        [16799.2871],\n", "        [ 7235.6304],\n", "        [50274.7266],\n", "        [ 7601.3564],\n", "        [ 3408.8757],\n", "        [ 1998.3617],\n", "        [ 8818.4814],\n", "        [ 5886.5854],\n", "        [ 3814.4329],\n", "        [ 2143.5500],\n", "        [16358.7646],\n", "        [10598.7148],\n", "        [ 5484.7227],\n", "        [46780.6094],\n", "        [51191.4062],\n", "        [20313.2656],\n", "        [24873.4727],\n", "        [ 4161.5991],\n", "        [16711.6484],\n", "        [ 2364.0461],\n", "        [ 3277.4729],\n", "        [10987.8750],\n", "        [46803.8984],\n", "        [ 6286.8359],\n", "        [ 4158.4175]])\n"]}], "source": ["for xb, yb in train_loader:\n", "    print(\"inputs:\", xb)\n", "    print(\"targets:\", yb)\n", "    break"]}, {"cell_type": "markdown", "metadata": {"id": "ZwySpYOkRB9B"}, "source": ["Let's save our work by committing to Jovian."]}, {"cell_type": "code", "execution_count": 160, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 103}, "executionInfo": {"elapsed": 5968, "status": "ok", "timestamp": 1607753178418, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "6GeuNCv0RB9B", "outputId": "aabcdb4a-d009-44c8-aa77-12e1a17a109a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[jovian] Detected Colab notebook...\u001b[0m\n", "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n", "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n", "[jovian] Committed successfully! https://jovian.ai/anurag3301/02-insurance-linear-regression\u001b[0m\n"]}, {"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'https://jovian.ai/anurag3301/02-insurance-linear-regression'"]}, "execution_count": 160, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["jovian.commit(project=project_name, environment=None)"]}, {"cell_type": "markdown", "metadata": {"id": "nThVgvByRB9B"}, "source": ["## Step 3: Create a Linear Regression Model\n", "\n", "Our model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment). \n"]}, {"cell_type": "code", "execution_count": 161, "metadata": {"executionInfo": {"elapsed": 919, "status": "ok", "timestamp": 1607753196283, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "T3BRWOwyRB9B"}, "outputs": [], "source": ["input_size = len(input_cols)\n", "output_size = len(output_cols)"]}, {"cell_type": "markdown", "metadata": {"id": "csIvuBcnRB9C"}, "source": ["**Q: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**\n", "\n", "Hint: Think carefully about picking a good loss fuction (it's not cross entropy). Maybe try 2-3 of them and see which one works best. See https://pytorch.org/docs/stable/nn.functional.html#loss-functions"]}, {"cell_type": "code", "execution_count": 162, "metadata": {"executionInfo": {"elapsed": 1424, "status": "ok", "timestamp": 1607753196819, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "Fc93QBnORB9C"}, "outputs": [], "source": ["class InsuranceModel(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.linear = nn.Linear(input_size, output_size)                  # fill this (hint: use input_size & output_size defined above)\n", "        \n", "    def forward(self, xb):\n", "        out = self.linear(xb)                          # fill this\n", "        return out\n", "    \n", "    def training_step(self, batch):\n", "        inputs, targets = batch \n", "        # Generate predictions\n", "        out = self(inputs)          \n", "        # Calcuate loss\n", "        loss = F.mse_loss(out, targets)                          # fill this\n", "        return loss\n", "    \n", "    def validation_step(self, batch):\n", "        inputs, targets = batch\n", "        # Generate predictions\n", "        out = self(inputs)\n", "        # Calculate loss\n", "        loss = F.mse_loss(out, targets)                           # fill this    \n", "        return {'val_loss': loss.detach()}\n", "        \n", "    def validation_epoch_end(self, outputs):\n", "        batch_losses = [x['val_loss'] for x in outputs]\n", "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n", "        return {'val_loss': epoch_loss.item()}\n", "    \n", "    def epoch_end(self, epoch, result, num_epochs):\n", "        # Print result every 20th epoch\n", "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n", "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"]}, {"cell_type": "markdown", "metadata": {"id": "RTPBJR9NRB9C"}, "source": ["Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`."]}, {"cell_type": "code", "execution_count": 163, "metadata": {"executionInfo": {"elapsed": 1418, "status": "ok", "timestamp": 1607753196822, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "M_LtEh6QRB9D"}, "outputs": [], "source": ["model = InsuranceModel()"]}, {"cell_type": "markdown", "metadata": {"id": "RiIwCmFjRB9D"}, "source": ["Let's check out the weights and biases of the model using `model.parameters`."]}, {"cell_type": "code", "execution_count": 164, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1407, "status": "ok", "timestamp": 1607753196824, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "zN5_adi_RB9D", "outputId": "c9449f13-6f53-4d28-cc47-b1e9f6d3eaf3"}, "outputs": [{"data": {"text/plain": ["[Parameter containing:\n", " tensor([[-0.3201,  0.3886,  0.3905, -0.2959, -0.2323, -0.1969]],\n", "        requires_grad=True), Parameter containing:\n", " tensor([-0.2601], requires_grad=True)]"]}, "execution_count": 164, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["list(model.parameters())"]}, {"cell_type": "markdown", "metadata": {"id": "7Dje67M5RB9D"}, "source": ["One final commit before we train the model."]}, {"cell_type": "code", "execution_count": 165, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 0}, "executionInfo": {"elapsed": 5404, "status": "ok", "timestamp": 1607753200832, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "BGMsJHUmRB9E", "outputId": "8ca7411d-f864-4974-9754-29a3cb351dc8"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[jovian] Detected Colab notebook...\u001b[0m\n", "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n", "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n", "[jovian] Committed successfully! https://jovian.ai/anurag3301/02-insurance-linear-regression\u001b[0m\n"]}, {"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'https://jovian.ai/anurag3301/02-insurance-linear-regression'"]}, "execution_count": 165, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["jovian.commit(project=project_name, environment=None)"]}, {"cell_type": "markdown", "metadata": {"id": "UMT5WgXbRB9E"}, "source": ["## Step 4: Train the model to fit the data\n", "\n", "To train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem."]}, {"cell_type": "code", "execution_count": 197, "metadata": {"executionInfo": {"elapsed": 957, "status": "ok", "timestamp": 1607755675500, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "nfaw6N4rRB9E"}, "outputs": [], "source": ["def evaluate(model, val_loader):\n", "    outputs = [model.validation_step(batch) for batch in val_loader]\n", "    return model.validation_epoch_end(outputs)\n", "\n", "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n", "    history = []\n", "    optimizer = opt_func(model.parameters(), lr)\n", "    for epoch in range(epochs):\n", "        # Training Phase \n", "        for batch in train_loader:\n", "            loss = model.training_step(batch)\n", "            loss.backward()\n", "            optimizer.step()\n", "            optimizer.zero_grad()\n", "        # Validation phase\n", "        result = evaluate(model, val_loader)\n", "        model.epoch_end(epoch, result, epochs)\n", "        history.append(result)\n", "    return history"]}, {"cell_type": "markdown", "metadata": {"id": "6hIc5xYCRB9E"}, "source": ["**Q: Use the `evaluate` function to calculate the loss on the validation set before training.**"]}, {"cell_type": "code", "execution_count": 198, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1420, "status": "ok", "timestamp": 1607755675986, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "beL8xXnxRB9E", "outputId": "c24893dc-c391-4bc3-b568-469bd7aeaad1"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'val_loss': 27569044.0}\n"]}], "source": ["result = evaluate(model, val_loader) # Use the the evaluate function\n", "print(result)"]}, {"cell_type": "markdown", "metadata": {"id": "gVrGBz7TRB9F"}, "source": ["\n", "We are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible."]}, {"cell_type": "markdown", "metadata": {"id": "NuGwoxcoRB9F"}, "source": ["**Q: Train the model 4-5 times with different learning rates & for different number of epochs.**\n", "\n", "Hint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works."]}, {"cell_type": "code", "execution_count": 199, "metadata": {"executionInfo": {"elapsed": 1409, "status": "ok", "timestamp": 1607755675989, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "c6XH80VKUeYR"}, "outputs": [], "source": ["model = InsuranceModel()"]}, {"cell_type": "code", "execution_count": 200, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 18072, "status": "ok", "timestamp": 1607755692665, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "FlvMZaUBRB9F", "outputId": "7c153dae-ca6d-4f44-cfba-dec250f736a9"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch [20], val_loss: 124562840.0000\n", "Epoch [40], val_loss: 137630272.0000\n", "Epoch [60], val_loss: 121388392.0000\n", "Epoch [80], val_loss: 135193024.0000\n", "Epoch [100], val_loss: 135985808.0000\n", "Epoch [120], val_loss: 125477880.0000\n", "Epoch [140], val_loss: 114577928.0000\n", "Epoch [160], val_loss: 123990240.0000\n", "Epoch [180], val_loss: 117851400.0000\n", "Epoch [200], val_loss: 109612888.0000\n", "Epoch [220], val_loss: 137076208.0000\n", "Epoch [240], val_loss: 115094944.0000\n", "Epoch [260], val_loss: 113507800.0000\n", "Epoch [280], val_loss: 104494600.0000\n", "Epoch [300], val_loss: 113768144.0000\n", "Epoch [320], val_loss: 100894328.0000\n", "Epoch [340], val_loss: 109321080.0000\n", "Epoch [360], val_loss: 99041112.0000\n", "Epoch [380], val_loss: 98300928.0000\n", "Epoch [400], val_loss: 135058848.0000\n", "Epoch [420], val_loss: 94775088.0000\n", "Epoch [440], val_loss: 158186176.0000\n", "Epoch [460], val_loss: 93196288.0000\n", "Epoch [480], val_loss: 97866968.0000\n", "Epoch [500], val_loss: 90031248.0000\n", "Epoch [520], val_loss: 107297040.0000\n", "Epoch [540], val_loss: 94243288.0000\n", "Epoch [560], val_loss: 86738776.0000\n", "Epoch [580], val_loss: 99099336.0000\n", "Epoch [600], val_loss: 88743624.0000\n", "Epoch [620], val_loss: 112022288.0000\n", "Epoch [640], val_loss: 84370128.0000\n", "Epoch [660], val_loss: 113564816.0000\n", "Epoch [680], val_loss: 81625712.0000\n", "Epoch [700], val_loss: 112250520.0000\n", "Epoch [720], val_loss: 89650016.0000\n", "Epoch [740], val_loss: 80577912.0000\n", "Epoch [760], val_loss: 83859384.0000\n", "Epoch [780], val_loss: 86154976.0000\n", "Epoch [800], val_loss: 108138432.0000\n", "Epoch [820], val_loss: 83356624.0000\n", "Epoch [840], val_loss: 83241904.0000\n", "Epoch [860], val_loss: 73483320.0000\n", "Epoch [880], val_loss: 72866232.0000\n", "Epoch [900], val_loss: 80325240.0000\n", "Epoch [920], val_loss: 70929376.0000\n", "Epoch [940], val_loss: 70839864.0000\n", "Epoch [960], val_loss: 114190856.0000\n", "Epoch [980], val_loss: 89291768.0000\n", "Epoch [1000], val_loss: 77220400.0000\n"]}], "source": ["epochs = 1000\n", "lr = 1e-4\n", "history1 = fit(epochs, lr, model, train_loader, val_loader)"]}, {"cell_type": "code", "execution_count": 201, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 184028, "status": "ok", "timestamp": 1607755858634, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "R_6FaUu3RB9F", "outputId": "6e121189-12f5-49fa-a17a-756d09e116c7"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch [20], val_loss: 69348760.0000\n", "Epoch [40], val_loss: 69847480.0000\n", "Epoch [60], val_loss: 107042536.0000\n", "Epoch [80], val_loss: 66575576.0000\n", "Epoch [100], val_loss: 73511480.0000\n", "Epoch [120], val_loss: 76269808.0000\n", "Epoch [140], val_loss: 75803928.0000\n", "Epoch [160], val_loss: 68048256.0000\n", "Epoch [180], val_loss: 62562164.0000\n", "Epoch [200], val_loss: 62925484.0000\n", "Epoch [220], val_loss: 67656624.0000\n", "Epoch [240], val_loss: 64605664.0000\n", "Epoch [260], val_loss: 63501724.0000\n", "Epoch [280], val_loss: 61264220.0000\n", "Epoch [300], val_loss: 109865720.0000\n", "Epoch [320], val_loss: 59715920.0000\n", "Epoch [340], val_loss: 58879748.0000\n", "Epoch [360], val_loss: 58330024.0000\n", "Epoch [380], val_loss: 61562736.0000\n", "Epoch [400], val_loss: 57848568.0000\n", "Epoch [420], val_loss: 58328976.0000\n", "Epoch [440], val_loss: 57633052.0000\n", "Epoch [460], val_loss: 56347120.0000\n", "Epoch [480], val_loss: 63108628.0000\n", "Epoch [500], val_loss: 63535520.0000\n", "Epoch [520], val_loss: 60512320.0000\n", "Epoch [540], val_loss: 55973788.0000\n", "Epoch [560], val_loss: 68768656.0000\n", "Epoch [580], val_loss: 52694408.0000\n", "Epoch [600], val_loss: 51758892.0000\n", "Epoch [620], val_loss: 52713080.0000\n", "Epoch [640], val_loss: 62959340.0000\n", "Epoch [660], val_loss: 52913884.0000\n", "Epoch [680], val_loss: 65664028.0000\n", "Epoch [700], val_loss: 50155212.0000\n", "Epoch [720], val_loss: 101441400.0000\n", "Epoch [740], val_loss: 49923004.0000\n", "Epoch [760], val_loss: 50065864.0000\n", "Epoch [780], val_loss: 54932416.0000\n", "Epoch [800], val_loss: 49117116.0000\n", "Epoch [820], val_loss: 50215036.0000\n", "Epoch [840], val_loss: 51088932.0000\n", "Epoch [860], val_loss: 49911032.0000\n", "Epoch [880], val_loss: 50577508.0000\n", "Epoch [900], val_loss: 47719312.0000\n", "Epoch [920], val_loss: 70013224.0000\n", "Epoch [940], val_loss: 58816280.0000\n", "Epoch [960], val_loss: 46254576.0000\n", "Epoch [980], val_loss: 81233632.0000\n", "Epoch [1000], val_loss: 45778616.0000\n", "Epoch [1020], val_loss: 45982860.0000\n", "Epoch [1040], val_loss: 54670308.0000\n", "Epoch [1060], val_loss: 46178396.0000\n", "Epoch [1080], val_loss: 46983912.0000\n", "Epoch [1100], val_loss: 44842164.0000\n", "Epoch [1120], val_loss: 44452384.0000\n", "Epoch [1140], val_loss: 76759824.0000\n", "Epoch [1160], val_loss: 44529540.0000\n", "Epoch [1180], val_loss: 53234388.0000\n", "Epoch [1200], val_loss: 56828388.0000\n", "Epoch [1220], val_loss: 42515080.0000\n", "Epoch [1240], val_loss: 42688628.0000\n", "Epoch [1260], val_loss: 44666660.0000\n", "Epoch [1280], val_loss: 53036984.0000\n", "Epoch [1300], val_loss: 46148436.0000\n", "Epoch [1320], val_loss: 42815040.0000\n", "Epoch [1340], val_loss: 42166916.0000\n", "Epoch [1360], val_loss: 41214832.0000\n", "Epoch [1380], val_loss: 62061240.0000\n", "Epoch [1400], val_loss: 49601800.0000\n", "Epoch [1420], val_loss: 45738464.0000\n", "Epoch [1440], val_loss: 40661576.0000\n", "Epoch [1460], val_loss: 40580508.0000\n", "Epoch [1480], val_loss: 44207728.0000\n", "Epoch [1500], val_loss: 43488412.0000\n", "Epoch [1520], val_loss: 40340696.0000\n", "Epoch [1540], val_loss: 39581712.0000\n", "Epoch [1560], val_loss: 39548832.0000\n", "Epoch [1580], val_loss: 39848560.0000\n", "Epoch [1600], val_loss: 45660368.0000\n", "Epoch [1620], val_loss: 39317592.0000\n", "Epoch [1640], val_loss: 38812880.0000\n", "Epoch [1660], val_loss: 43684840.0000\n", "Epoch [1680], val_loss: 38498820.0000\n", "Epoch [1700], val_loss: 41014300.0000\n", "Epoch [1720], val_loss: 46932324.0000\n", "Epoch [1740], val_loss: 39330380.0000\n", "Epoch [1760], val_loss: 37561196.0000\n", "Epoch [1780], val_loss: 37814352.0000\n", "Epoch [1800], val_loss: 45769692.0000\n", "Epoch [1820], val_loss: 43985864.0000\n", "Epoch [1840], val_loss: 38026328.0000\n", "Epoch [1860], val_loss: 42217324.0000\n", "Epoch [1880], val_loss: 36245340.0000\n", "Epoch [1900], val_loss: 37429856.0000\n", "Epoch [1920], val_loss: 43192292.0000\n", "Epoch [1940], val_loss: 39126968.0000\n", "Epoch [1960], val_loss: 43775112.0000\n", "Epoch [1980], val_loss: 36044312.0000\n", "Epoch [2000], val_loss: 36432108.0000\n", "Epoch [2020], val_loss: 35941392.0000\n", "Epoch [2040], val_loss: 37185232.0000\n", "Epoch [2060], val_loss: 47208884.0000\n", "Epoch [2080], val_loss: 36099464.0000\n", "Epoch [2100], val_loss: 38656456.0000\n", "Epoch [2120], val_loss: 36532444.0000\n", "Epoch [2140], val_loss: 35211544.0000\n", "Epoch [2160], val_loss: 37071140.0000\n", "Epoch [2180], val_loss: 46134032.0000\n", "Epoch [2200], val_loss: 36825768.0000\n", "Epoch [2220], val_loss: 36855576.0000\n", "Epoch [2240], val_loss: 34996280.0000\n", "Epoch [2260], val_loss: 36969152.0000\n", "Epoch [2280], val_loss: 35523120.0000\n", "Epoch [2300], val_loss: 36490852.0000\n", "Epoch [2320], val_loss: 37346308.0000\n", "Epoch [2340], val_loss: 34990600.0000\n", "Epoch [2360], val_loss: 34680520.0000\n", "Epoch [2380], val_loss: 70261920.0000\n", "Epoch [2400], val_loss: 52744296.0000\n", "Epoch [2420], val_loss: 33903696.0000\n", "Epoch [2440], val_loss: 34283036.0000\n", "Epoch [2460], val_loss: 34030584.0000\n", "Epoch [2480], val_loss: 34702384.0000\n", "Epoch [2500], val_loss: 34343068.0000\n", "Epoch [2520], val_loss: 34348708.0000\n", "Epoch [2540], val_loss: 33698920.0000\n", "Epoch [2560], val_loss: 59440452.0000\n", "Epoch [2580], val_loss: 42278172.0000\n", "Epoch [2600], val_loss: 33067312.0000\n", "Epoch [2620], val_loss: 34112792.0000\n", "Epoch [2640], val_loss: 41920304.0000\n", "Epoch [2660], val_loss: 44481756.0000\n", "Epoch [2680], val_loss: 34184680.0000\n", "Epoch [2700], val_loss: 33457024.0000\n", "Epoch [2720], val_loss: 32631950.0000\n", "Epoch [2740], val_loss: 32591840.0000\n", "Epoch [2760], val_loss: 32774576.0000\n", "Epoch [2780], val_loss: 33119336.0000\n", "Epoch [2800], val_loss: 37035756.0000\n", "Epoch [2820], val_loss: 32514226.0000\n", "Epoch [2840], val_loss: 34221064.0000\n", "Epoch [2860], val_loss: 32525332.0000\n", "Epoch [2880], val_loss: 33663552.0000\n", "Epoch [2900], val_loss: 42338004.0000\n", "Epoch [2920], val_loss: 51480884.0000\n", "Epoch [2940], val_loss: 32336180.0000\n", "Epoch [2960], val_loss: 33770152.0000\n", "Epoch [2980], val_loss: 34240652.0000\n", "Epoch [3000], val_loss: 32918302.0000\n", "Epoch [3020], val_loss: 31815714.0000\n", "Epoch [3040], val_loss: 43881496.0000\n", "Epoch [3060], val_loss: 37339800.0000\n", "Epoch [3080], val_loss: 32754026.0000\n", "Epoch [3100], val_loss: 41756180.0000\n", "Epoch [3120], val_loss: 32208980.0000\n", "Epoch [3140], val_loss: 37770244.0000\n", "Epoch [3160], val_loss: 31377412.0000\n", "Epoch [3180], val_loss: 60532624.0000\n", "Epoch [3200], val_loss: 31960664.0000\n", "Epoch [3220], val_loss: 31788326.0000\n", "Epoch [3240], val_loss: 40030252.0000\n", "Epoch [3260], val_loss: 31376054.0000\n", "Epoch [3280], val_loss: 32872226.0000\n", "Epoch [3300], val_loss: 31266998.0000\n", "Epoch [3320], val_loss: 31004270.0000\n", "Epoch [3340], val_loss: 36104680.0000\n", "Epoch [3360], val_loss: 31406548.0000\n", "Epoch [3380], val_loss: 30944964.0000\n", "Epoch [3400], val_loss: 31044464.0000\n", "Epoch [3420], val_loss: 44453428.0000\n", "Epoch [3440], val_loss: 30554864.0000\n", "Epoch [3460], val_loss: 32496714.0000\n", "Epoch [3480], val_loss: 30337028.0000\n", "Epoch [3500], val_loss: 31808000.0000\n", "Epoch [3520], val_loss: 32241820.0000\n", "Epoch [3540], val_loss: 39307000.0000\n", "Epoch [3560], val_loss: 35840316.0000\n", "Epoch [3580], val_loss: 30469970.0000\n", "Epoch [3600], val_loss: 32235634.0000\n", "Epoch [3620], val_loss: 30659292.0000\n", "Epoch [3640], val_loss: 32062444.0000\n", "Epoch [3660], val_loss: 37203132.0000\n", "Epoch [3680], val_loss: 31302794.0000\n", "Epoch [3700], val_loss: 32838272.0000\n", "Epoch [3720], val_loss: 31057792.0000\n", "Epoch [3740], val_loss: 41886832.0000\n", "Epoch [3760], val_loss: 31951880.0000\n", "Epoch [3780], val_loss: 35897812.0000\n", "Epoch [3800], val_loss: 44753852.0000\n", "Epoch [3820], val_loss: 31378040.0000\n", "Epoch [3840], val_loss: 35738980.0000\n", "Epoch [3860], val_loss: 48391304.0000\n", "Epoch [3880], val_loss: 34054156.0000\n", "Epoch [3900], val_loss: 29977158.0000\n", "Epoch [3920], val_loss: 30072804.0000\n", "Epoch [3940], val_loss: 32860870.0000\n", "Epoch [3960], val_loss: 30222686.0000\n", "Epoch [3980], val_loss: 42815520.0000\n", "Epoch [4000], val_loss: 30063672.0000\n", "Epoch [4020], val_loss: 30042932.0000\n", "Epoch [4040], val_loss: 29668596.0000\n", "Epoch [4060], val_loss: 29740592.0000\n", "Epoch [4080], val_loss: 31075890.0000\n", "Epoch [4100], val_loss: 44465840.0000\n", "Epoch [4120], val_loss: 30862644.0000\n", "Epoch [4140], val_loss: 30374920.0000\n", "Epoch [4160], val_loss: 29671726.0000\n", "Epoch [4180], val_loss: 31028866.0000\n", "Epoch [4200], val_loss: 33909748.0000\n", "Epoch [4220], val_loss: 30626630.0000\n", "Epoch [4240], val_loss: 29486800.0000\n", "Epoch [4260], val_loss: 31489384.0000\n", "Epoch [4280], val_loss: 29551224.0000\n", "Epoch [4300], val_loss: 31416636.0000\n", "Epoch [4320], val_loss: 29633840.0000\n", "Epoch [4340], val_loss: 31400960.0000\n", "Epoch [4360], val_loss: 29765818.0000\n", "Epoch [4380], val_loss: 51874724.0000\n", "Epoch [4400], val_loss: 31214674.0000\n", "Epoch [4420], val_loss: 31006066.0000\n", "Epoch [4440], val_loss: 32149480.0000\n", "Epoch [4460], val_loss: 30090638.0000\n", "Epoch [4480], val_loss: 29277952.0000\n", "Epoch [4500], val_loss: 31740020.0000\n", "Epoch [4520], val_loss: 29121662.0000\n", "Epoch [4540], val_loss: 30553502.0000\n", "Epoch [4560], val_loss: 31472434.0000\n", "Epoch [4580], val_loss: 29316490.0000\n", "Epoch [4600], val_loss: 30056350.0000\n", "Epoch [4620], val_loss: 30820604.0000\n", "Epoch [4640], val_loss: 29438442.0000\n", "Epoch [4660], val_loss: 31225008.0000\n", "Epoch [4680], val_loss: 29448410.0000\n", "Epoch [4700], val_loss: 32118992.0000\n", "Epoch [4720], val_loss: 29296106.0000\n", "Epoch [4740], val_loss: 36663436.0000\n", "Epoch [4760], val_loss: 37423256.0000\n", "Epoch [4780], val_loss: 29649494.0000\n", "Epoch [4800], val_loss: 29278996.0000\n", "Epoch [4820], val_loss: 29936714.0000\n", "Epoch [4840], val_loss: 42863552.0000\n", "Epoch [4860], val_loss: 29254188.0000\n", "Epoch [4880], val_loss: 31247636.0000\n", "Epoch [4900], val_loss: 29890436.0000\n", "Epoch [4920], val_loss: 29913034.0000\n", "Epoch [4940], val_loss: 31519684.0000\n", "Epoch [4960], val_loss: 30843862.0000\n", "Epoch [4980], val_loss: 29883180.0000\n", "Epoch [5000], val_loss: 29624350.0000\n", "Epoch [5020], val_loss: 39190516.0000\n", "Epoch [5040], val_loss: 29231010.0000\n", "Epoch [5060], val_loss: 35412508.0000\n", "Epoch [5080], val_loss: 28752614.0000\n", "Epoch [5100], val_loss: 32202054.0000\n", "Epoch [5120], val_loss: 28781436.0000\n", "Epoch [5140], val_loss: 30264986.0000\n", "Epoch [5160], val_loss: 41880872.0000\n", "Epoch [5180], val_loss: 39276216.0000\n", "Epoch [5200], val_loss: 28858998.0000\n", "Epoch [5220], val_loss: 46642404.0000\n", "Epoch [5240], val_loss: 30640378.0000\n", "Epoch [5260], val_loss: 28948608.0000\n", "Epoch [5280], val_loss: 35499012.0000\n", "Epoch [5300], val_loss: 28841014.0000\n", "Epoch [5320], val_loss: 28829280.0000\n", "Epoch [5340], val_loss: 31459208.0000\n", "Epoch [5360], val_loss: 33658268.0000\n", "Epoch [5380], val_loss: 28839890.0000\n", "Epoch [5400], val_loss: 37301616.0000\n", "Epoch [5420], val_loss: 32308094.0000\n", "Epoch [5440], val_loss: 28463724.0000\n", "Epoch [5460], val_loss: 28670334.0000\n", "Epoch [5480], val_loss: 28586788.0000\n", "Epoch [5500], val_loss: 28601014.0000\n", "Epoch [5520], val_loss: 30846262.0000\n", "Epoch [5540], val_loss: 32274738.0000\n", "Epoch [5560], val_loss: 28638746.0000\n", "Epoch [5580], val_loss: 28472246.0000\n", "Epoch [5600], val_loss: 32268568.0000\n", "Epoch [5620], val_loss: 29857848.0000\n", "Epoch [5640], val_loss: 30550318.0000\n", "Epoch [5660], val_loss: 55286040.0000\n", "Epoch [5680], val_loss: 28464684.0000\n", "Epoch [5700], val_loss: 32792832.0000\n", "Epoch [5720], val_loss: 31541364.0000\n", "Epoch [5740], val_loss: 31292156.0000\n", "Epoch [5760], val_loss: 28380110.0000\n", "Epoch [5780], val_loss: 28617548.0000\n", "Epoch [5800], val_loss: 28123208.0000\n", "Epoch [5820], val_loss: 28446618.0000\n", "Epoch [5840], val_loss: 42592664.0000\n", "Epoch [5860], val_loss: 30310518.0000\n", "Epoch [5880], val_loss: 30054470.0000\n", "Epoch [5900], val_loss: 29838544.0000\n", "Epoch [5920], val_loss: 28083832.0000\n", "Epoch [5940], val_loss: 30560378.0000\n", "Epoch [5960], val_loss: 31691092.0000\n", "Epoch [5980], val_loss: 28206874.0000\n", "Epoch [6000], val_loss: 28244516.0000\n", "Epoch [6020], val_loss: 34893996.0000\n", "Epoch [6040], val_loss: 30059614.0000\n", "Epoch [6060], val_loss: 28332974.0000\n", "Epoch [6080], val_loss: 46579484.0000\n", "Epoch [6100], val_loss: 37497124.0000\n", "Epoch [6120], val_loss: 34053024.0000\n", "Epoch [6140], val_loss: 28615042.0000\n", "Epoch [6160], val_loss: 28345058.0000\n", "Epoch [6180], val_loss: 31558604.0000\n", "Epoch [6200], val_loss: 33042498.0000\n", "Epoch [6220], val_loss: 29303060.0000\n", "Epoch [6240], val_loss: 28375346.0000\n", "Epoch [6260], val_loss: 28178834.0000\n", "Epoch [6280], val_loss: 37535960.0000\n", "Epoch [6300], val_loss: 29049082.0000\n", "Epoch [6320], val_loss: 28308630.0000\n", "Epoch [6340], val_loss: 36211144.0000\n", "Epoch [6360], val_loss: 28310420.0000\n", "Epoch [6380], val_loss: 28122496.0000\n", "Epoch [6400], val_loss: 31223914.0000\n", "Epoch [6420], val_loss: 34223564.0000\n", "Epoch [6440], val_loss: 44077248.0000\n", "Epoch [6460], val_loss: 28893172.0000\n", "Epoch [6480], val_loss: 38534416.0000\n", "Epoch [6500], val_loss: 32423630.0000\n", "Epoch [6520], val_loss: 30504290.0000\n", "Epoch [6540], val_loss: 39294252.0000\n", "Epoch [6560], val_loss: 36772296.0000\n", "Epoch [6580], val_loss: 28047070.0000\n", "Epoch [6600], val_loss: 37017124.0000\n", "Epoch [6620], val_loss: 29095838.0000\n", "Epoch [6640], val_loss: 29893962.0000\n", "Epoch [6660], val_loss: 29950156.0000\n", "Epoch [6680], val_loss: 29988948.0000\n", "Epoch [6700], val_loss: 30114276.0000\n", "Epoch [6720], val_loss: 30978312.0000\n", "Epoch [6740], val_loss: 28996884.0000\n", "Epoch [6760], val_loss: 30476026.0000\n", "Epoch [6780], val_loss: 28989132.0000\n", "Epoch [6800], val_loss: 31501360.0000\n", "Epoch [6820], val_loss: 27898394.0000\n", "Epoch [6840], val_loss: 27728340.0000\n", "Epoch [6860], val_loss: 31045438.0000\n", "Epoch [6880], val_loss: 27805918.0000\n", "Epoch [6900], val_loss: 28165280.0000\n", "Epoch [6920], val_loss: 31356360.0000\n", "Epoch [6940], val_loss: 30273570.0000\n", "Epoch [6960], val_loss: 37043648.0000\n", "Epoch [6980], val_loss: 30006222.0000\n", "Epoch [7000], val_loss: 30684022.0000\n", "Epoch [7020], val_loss: 27742482.0000\n", "Epoch [7040], val_loss: 31713574.0000\n", "Epoch [7060], val_loss: 28024662.0000\n", "Epoch [7080], val_loss: 28756342.0000\n", "Epoch [7100], val_loss: 28782174.0000\n", "Epoch [7120], val_loss: 28585670.0000\n", "Epoch [7140], val_loss: 28363938.0000\n", "Epoch [7160], val_loss: 34693484.0000\n", "Epoch [7180], val_loss: 27885194.0000\n", "Epoch [7200], val_loss: 28057454.0000\n", "Epoch [7220], val_loss: 27854842.0000\n", "Epoch [7240], val_loss: 29860214.0000\n", "Epoch [7260], val_loss: 29231568.0000\n", "Epoch [7280], val_loss: 44104392.0000\n", "Epoch [7300], val_loss: 27842198.0000\n", "Epoch [7320], val_loss: 27747124.0000\n", "Epoch [7340], val_loss: 28140496.0000\n", "Epoch [7360], val_loss: 27767010.0000\n", "Epoch [7380], val_loss: 31491026.0000\n", "Epoch [7400], val_loss: 44666020.0000\n", "Epoch [7420], val_loss: 28436810.0000\n", "Epoch [7440], val_loss: 28682754.0000\n", "Epoch [7460], val_loss: 28052088.0000\n", "Epoch [7480], val_loss: 35852616.0000\n", "Epoch [7500], val_loss: 27808770.0000\n", "Epoch [7520], val_loss: 29401724.0000\n", "Epoch [7540], val_loss: 30600240.0000\n", "Epoch [7560], val_loss: 31020068.0000\n", "Epoch [7580], val_loss: 37441912.0000\n", "Epoch [7600], val_loss: 37879180.0000\n", "Epoch [7620], val_loss: 31170180.0000\n", "Epoch [7640], val_loss: 29622538.0000\n", "Epoch [7660], val_loss: 28724510.0000\n", "Epoch [7680], val_loss: 29236456.0000\n", "Epoch [7700], val_loss: 27833826.0000\n", "Epoch [7720], val_loss: 27531048.0000\n", "Epoch [7740], val_loss: 27681308.0000\n", "Epoch [7760], val_loss: 27530446.0000\n", "Epoch [7780], val_loss: 29894356.0000\n", "Epoch [7800], val_loss: 28557170.0000\n", "Epoch [7820], val_loss: 31924296.0000\n", "Epoch [7840], val_loss: 27831150.0000\n", "Epoch [7860], val_loss: 35750236.0000\n", "Epoch [7880], val_loss: 34104136.0000\n", "Epoch [7900], val_loss: 30152754.0000\n", "Epoch [7920], val_loss: 37701224.0000\n", "Epoch [7940], val_loss: 28287094.0000\n", "Epoch [7960], val_loss: 28736600.0000\n", "Epoch [7980], val_loss: 31372518.0000\n", "Epoch [8000], val_loss: 36096940.0000\n", "Epoch [8020], val_loss: 30064956.0000\n", "Epoch [8040], val_loss: 33254750.0000\n", "Epoch [8060], val_loss: 28334884.0000\n", "Epoch [8080], val_loss: 27765398.0000\n", "Epoch [8100], val_loss: 27771616.0000\n", "Epoch [8120], val_loss: 27313954.0000\n", "Epoch [8140], val_loss: 27328928.0000\n", "Epoch [8160], val_loss: 27575904.0000\n", "Epoch [8180], val_loss: 28476554.0000\n", "Epoch [8200], val_loss: 29116162.0000\n", "Epoch [8220], val_loss: 29234884.0000\n", "Epoch [8240], val_loss: 27999004.0000\n", "Epoch [8260], val_loss: 27481202.0000\n", "Epoch [8280], val_loss: 27616698.0000\n", "Epoch [8300], val_loss: 32705480.0000\n", "Epoch [8320], val_loss: 30505168.0000\n", "Epoch [8340], val_loss: 69468784.0000\n", "Epoch [8360], val_loss: 27464736.0000\n", "Epoch [8380], val_loss: 29099648.0000\n", "Epoch [8400], val_loss: 27507660.0000\n", "Epoch [8420], val_loss: 27578498.0000\n", "Epoch [8440], val_loss: 32582148.0000\n", "Epoch [8460], val_loss: 27606002.0000\n", "Epoch [8480], val_loss: 32138366.0000\n", "Epoch [8500], val_loss: 28540132.0000\n", "Epoch [8520], val_loss: 33394908.0000\n", "Epoch [8540], val_loss: 28378546.0000\n", "Epoch [8560], val_loss: 29117338.0000\n", "Epoch [8580], val_loss: 33675632.0000\n", "Epoch [8600], val_loss: 27313334.0000\n", "Epoch [8620], val_loss: 27924818.0000\n", "Epoch [8640], val_loss: 31865336.0000\n", "Epoch [8660], val_loss: 40558844.0000\n", "Epoch [8680], val_loss: 44002860.0000\n", "Epoch [8700], val_loss: 27813070.0000\n", "Epoch [8720], val_loss: 27249696.0000\n", "Epoch [8740], val_loss: 27593620.0000\n", "Epoch [8760], val_loss: 32960168.0000\n", "Epoch [8780], val_loss: 35242820.0000\n", "Epoch [8800], val_loss: 37637388.0000\n", "Epoch [8820], val_loss: 27607746.0000\n", "Epoch [8840], val_loss: 32144746.0000\n", "Epoch [8860], val_loss: 29393458.0000\n", "Epoch [8880], val_loss: 27156260.0000\n", "Epoch [8900], val_loss: 27961938.0000\n", "Epoch [8920], val_loss: 32394406.0000\n", "Epoch [8940], val_loss: 29341716.0000\n", "Epoch [8960], val_loss: 42699736.0000\n", "Epoch [8980], val_loss: 27169374.0000\n", "Epoch [9000], val_loss: 27977554.0000\n", "Epoch [9020], val_loss: 29874200.0000\n", "Epoch [9040], val_loss: 27195290.0000\n", "Epoch [9060], val_loss: 28527520.0000\n", "Epoch [9080], val_loss: 35506524.0000\n", "Epoch [9100], val_loss: 28113204.0000\n", "Epoch [9120], val_loss: 27338748.0000\n", "Epoch [9140], val_loss: 28267794.0000\n", "Epoch [9160], val_loss: 28013332.0000\n", "Epoch [9180], val_loss: 27305772.0000\n", "Epoch [9200], val_loss: 27322378.0000\n", "Epoch [9220], val_loss: 27907842.0000\n", "Epoch [9240], val_loss: 27207680.0000\n", "Epoch [9260], val_loss: 29270370.0000\n", "Epoch [9280], val_loss: 27261844.0000\n", "Epoch [9300], val_loss: 27347434.0000\n", "Epoch [9320], val_loss: 43021928.0000\n", "Epoch [9340], val_loss: 27305852.0000\n", "Epoch [9360], val_loss: 33688688.0000\n", "Epoch [9380], val_loss: 36460908.0000\n", "Epoch [9400], val_loss: 32933838.0000\n", "Epoch [9420], val_loss: 29354012.0000\n", "Epoch [9440], val_loss: 28550290.0000\n", "Epoch [9460], val_loss: 28508680.0000\n", "Epoch [9480], val_loss: 27192950.0000\n", "Epoch [9500], val_loss: 27848276.0000\n", "Epoch [9520], val_loss: 27741636.0000\n", "Epoch [9540], val_loss: 29706096.0000\n", "Epoch [9560], val_loss: 28446092.0000\n", "Epoch [9580], val_loss: 28662190.0000\n", "Epoch [9600], val_loss: 27320908.0000\n", "Epoch [9620], val_loss: 27187618.0000\n", "Epoch [9640], val_loss: 27372646.0000\n", "Epoch [9660], val_loss: 27139430.0000\n", "Epoch [9680], val_loss: 27666290.0000\n", "Epoch [9700], val_loss: 27544120.0000\n", "Epoch [9720], val_loss: 28955156.0000\n", "Epoch [9740], val_loss: 27886614.0000\n", "Epoch [9760], val_loss: 29528708.0000\n", "Epoch [9780], val_loss: 29854014.0000\n", "Epoch [9800], val_loss: 29274394.0000\n", "Epoch [9820], val_loss: 27125082.0000\n", "Epoch [9840], val_loss: 29082566.0000\n", "Epoch [9860], val_loss: 42664864.0000\n", "Epoch [9880], val_loss: 28384672.0000\n", "Epoch [9900], val_loss: 34054092.0000\n", "Epoch [9920], val_loss: 27112696.0000\n", "Epoch [9940], val_loss: 28327718.0000\n", "Epoch [9960], val_loss: 27081636.0000\n", "Epoch [9980], val_loss: 27181056.0000\n", "Epoch [10000], val_loss: 27119080.0000\n"]}], "source": ["epochs = 10000\n", "lr = 1e-4\n", "history2 = fit(epochs, lr, model, train_loader, val_loader)"]}, {"cell_type": "code", "execution_count": 202, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 350319, "status": "ok", "timestamp": 1607756024933, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "5B0LqX0nRB9F", "outputId": "9cddae5c-326f-44e1-ecbb-36246b605d8c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch [20], val_loss: 27475198.0000\n", "Epoch [40], val_loss: 27715510.0000\n", "Epoch [60], val_loss: 31812526.0000\n", "Epoch [80], val_loss: 27042096.0000\n", "Epoch [100], val_loss: 27880630.0000\n", "Epoch [120], val_loss: 39645920.0000\n", "Epoch [140], val_loss: 29144032.0000\n", "Epoch [160], val_loss: 28934934.0000\n", "Epoch [180], val_loss: 27255940.0000\n", "Epoch [200], val_loss: 27946870.0000\n", "Epoch [220], val_loss: 27249662.0000\n", "Epoch [240], val_loss: 28087186.0000\n", "Epoch [260], val_loss: 27159958.0000\n", "Epoch [280], val_loss: 27319594.0000\n", "Epoch [300], val_loss: 27780888.0000\n", "Epoch [320], val_loss: 26927382.0000\n", "Epoch [340], val_loss: 27275074.0000\n", "Epoch [360], val_loss: 27089540.0000\n", "Epoch [380], val_loss: 27888580.0000\n", "Epoch [400], val_loss: 28472438.0000\n", "Epoch [420], val_loss: 32461296.0000\n", "Epoch [440], val_loss: 27052532.0000\n", "Epoch [460], val_loss: 27132238.0000\n", "Epoch [480], val_loss: 27175502.0000\n", "Epoch [500], val_loss: 65910452.0000\n", "Epoch [520], val_loss: 30400368.0000\n", "Epoch [540], val_loss: 28562156.0000\n", "Epoch [560], val_loss: 27197340.0000\n", "Epoch [580], val_loss: 26877896.0000\n", "Epoch [600], val_loss: 31192462.0000\n", "Epoch [620], val_loss: 27130212.0000\n", "Epoch [640], val_loss: 43048696.0000\n", "Epoch [660], val_loss: 29614104.0000\n", "Epoch [680], val_loss: 43761628.0000\n", "Epoch [700], val_loss: 33580748.0000\n", "Epoch [720], val_loss: 32631492.0000\n", "Epoch [740], val_loss: 28819018.0000\n", "Epoch [760], val_loss: 27853644.0000\n", "Epoch [780], val_loss: 28014612.0000\n", "Epoch [800], val_loss: 39230988.0000\n", "Epoch [820], val_loss: 32707142.0000\n", "Epoch [840], val_loss: 32197922.0000\n", "Epoch [860], val_loss: 29051258.0000\n", "Epoch [880], val_loss: 27377518.0000\n", "Epoch [900], val_loss: 32187158.0000\n", "Epoch [920], val_loss: 28585752.0000\n", "Epoch [940], val_loss: 28194418.0000\n", "Epoch [960], val_loss: 27787158.0000\n", "Epoch [980], val_loss: 28781972.0000\n", "Epoch [1000], val_loss: 33102356.0000\n", "Epoch [1020], val_loss: 28765648.0000\n", "Epoch [1040], val_loss: 38764340.0000\n", "Epoch [1060], val_loss: 29697948.0000\n", "Epoch [1080], val_loss: 33349522.0000\n", "Epoch [1100], val_loss: 27634394.0000\n", "Epoch [1120], val_loss: 26885344.0000\n", "Epoch [1140], val_loss: 29264574.0000\n", "Epoch [1160], val_loss: 27353838.0000\n", "Epoch [1180], val_loss: 26777060.0000\n", "Epoch [1200], val_loss: 26797308.0000\n", "Epoch [1220], val_loss: 27432424.0000\n", "Epoch [1240], val_loss: 27159852.0000\n", "Epoch [1260], val_loss: 26775630.0000\n", "Epoch [1280], val_loss: 33265222.0000\n", "Epoch [1300], val_loss: 32576652.0000\n", "Epoch [1320], val_loss: 26783738.0000\n", "Epoch [1340], val_loss: 29420528.0000\n", "Epoch [1360], val_loss: 26891720.0000\n", "Epoch [1380], val_loss: 53751416.0000\n", "Epoch [1400], val_loss: 27436568.0000\n", "Epoch [1420], val_loss: 27780446.0000\n", "Epoch [1440], val_loss: 26901732.0000\n", "Epoch [1460], val_loss: 29164948.0000\n", "Epoch [1480], val_loss: 27544766.0000\n", "Epoch [1500], val_loss: 29212862.0000\n", "Epoch [1520], val_loss: 27205552.0000\n", "Epoch [1540], val_loss: 27156220.0000\n", "Epoch [1560], val_loss: 26608108.0000\n", "Epoch [1580], val_loss: 28528128.0000\n", "Epoch [1600], val_loss: 26739640.0000\n", "Epoch [1620], val_loss: 27972914.0000\n", "Epoch [1640], val_loss: 30170412.0000\n", "Epoch [1660], val_loss: 26944930.0000\n", "Epoch [1680], val_loss: 26815512.0000\n", "Epoch [1700], val_loss: 28291926.0000\n", "Epoch [1720], val_loss: 27639726.0000\n", "Epoch [1740], val_loss: 31658252.0000\n", "Epoch [1760], val_loss: 30753200.0000\n", "Epoch [1780], val_loss: 30563442.0000\n", "Epoch [1800], val_loss: 35472704.0000\n", "Epoch [1820], val_loss: 27273188.0000\n", "Epoch [1840], val_loss: 26689012.0000\n", "Epoch [1860], val_loss: 26823220.0000\n", "Epoch [1880], val_loss: 27396866.0000\n", "Epoch [1900], val_loss: 26730690.0000\n", "Epoch [1920], val_loss: 26935182.0000\n", "Epoch [1940], val_loss: 53522992.0000\n", "Epoch [1960], val_loss: 27444436.0000\n", "Epoch [1980], val_loss: 31739788.0000\n", "Epoch [2000], val_loss: 31534410.0000\n", "Epoch [2020], val_loss: 32405828.0000\n", "Epoch [2040], val_loss: 26793330.0000\n", "Epoch [2060], val_loss: 26805204.0000\n", "Epoch [2080], val_loss: 27907880.0000\n", "Epoch [2100], val_loss: 27250218.0000\n", "Epoch [2120], val_loss: 26898762.0000\n", "Epoch [2140], val_loss: 26953508.0000\n", "Epoch [2160], val_loss: 28363854.0000\n", "Epoch [2180], val_loss: 40036200.0000\n", "Epoch [2200], val_loss: 28430758.0000\n", "Epoch [2220], val_loss: 26919942.0000\n", "Epoch [2240], val_loss: 28465526.0000\n", "Epoch [2260], val_loss: 27007762.0000\n", "Epoch [2280], val_loss: 26960856.0000\n", "Epoch [2300], val_loss: 26556196.0000\n", "Epoch [2320], val_loss: 26626126.0000\n", "Epoch [2340], val_loss: 30642090.0000\n", "Epoch [2360], val_loss: 26644686.0000\n", "Epoch [2380], val_loss: 28287206.0000\n", "Epoch [2400], val_loss: 26913814.0000\n", "Epoch [2420], val_loss: 27635094.0000\n", "Epoch [2440], val_loss: 27431834.0000\n", "Epoch [2460], val_loss: 27332870.0000\n", "Epoch [2480], val_loss: 29366642.0000\n", "Epoch [2500], val_loss: 31594036.0000\n", "Epoch [2520], val_loss: 27252636.0000\n", "Epoch [2540], val_loss: 27173080.0000\n", "Epoch [2560], val_loss: 29916600.0000\n", "Epoch [2580], val_loss: 32939736.0000\n", "Epoch [2600], val_loss: 26869910.0000\n", "Epoch [2620], val_loss: 28205926.0000\n", "Epoch [2640], val_loss: 31284784.0000\n", "Epoch [2660], val_loss: 28446554.0000\n", "Epoch [2680], val_loss: 28726436.0000\n", "Epoch [2700], val_loss: 27665618.0000\n", "Epoch [2720], val_loss: 27226742.0000\n", "Epoch [2740], val_loss: 27374082.0000\n", "Epoch [2760], val_loss: 28634306.0000\n", "Epoch [2780], val_loss: 34892488.0000\n", "Epoch [2800], val_loss: 28666832.0000\n", "Epoch [2820], val_loss: 30197698.0000\n", "Epoch [2840], val_loss: 26623502.0000\n", "Epoch [2860], val_loss: 26482738.0000\n", "Epoch [2880], val_loss: 31691288.0000\n", "Epoch [2900], val_loss: 42390824.0000\n", "Epoch [2920], val_loss: 30035156.0000\n", "Epoch [2940], val_loss: 32486010.0000\n", "Epoch [2960], val_loss: 37790476.0000\n", "Epoch [2980], val_loss: 29614604.0000\n", "Epoch [3000], val_loss: 27275220.0000\n", "Epoch [3020], val_loss: 26994930.0000\n", "Epoch [3040], val_loss: 29941926.0000\n", "Epoch [3060], val_loss: 26699266.0000\n", "Epoch [3080], val_loss: 26696788.0000\n", "Epoch [3100], val_loss: 26879704.0000\n", "Epoch [3120], val_loss: 27261978.0000\n", "Epoch [3140], val_loss: 27896158.0000\n", "Epoch [3160], val_loss: 28400284.0000\n", "Epoch [3180], val_loss: 28967994.0000\n", "Epoch [3200], val_loss: 27180004.0000\n", "Epoch [3220], val_loss: 32149136.0000\n", "Epoch [3240], val_loss: 26999940.0000\n", "Epoch [3260], val_loss: 32382280.0000\n", "Epoch [3280], val_loss: 35337868.0000\n", "Epoch [3300], val_loss: 26766908.0000\n", "Epoch [3320], val_loss: 27075258.0000\n", "Epoch [3340], val_loss: 38819860.0000\n", "Epoch [3360], val_loss: 28261826.0000\n", "Epoch [3380], val_loss: 27726666.0000\n", "Epoch [3400], val_loss: 27021108.0000\n", "Epoch [3420], val_loss: 26483924.0000\n", "Epoch [3440], val_loss: 26526124.0000\n", "Epoch [3460], val_loss: 26934012.0000\n", "Epoch [3480], val_loss: 31320652.0000\n", "Epoch [3500], val_loss: 29764576.0000\n", "Epoch [3520], val_loss: 27884504.0000\n", "Epoch [3540], val_loss: 33037458.0000\n", "Epoch [3560], val_loss: 32923256.0000\n", "Epoch [3580], val_loss: 33029084.0000\n", "Epoch [3600], val_loss: 27449976.0000\n", "Epoch [3620], val_loss: 27180852.0000\n", "Epoch [3640], val_loss: 26466966.0000\n", "Epoch [3660], val_loss: 35534648.0000\n", "Epoch [3680], val_loss: 26907156.0000\n", "Epoch [3700], val_loss: 26853908.0000\n", "Epoch [3720], val_loss: 27913220.0000\n", "Epoch [3740], val_loss: 28090848.0000\n", "Epoch [3760], val_loss: 34096360.0000\n", "Epoch [3780], val_loss: 33774856.0000\n", "Epoch [3800], val_loss: 27203942.0000\n", "Epoch [3820], val_loss: 27266414.0000\n", "Epoch [3840], val_loss: 28695032.0000\n", "Epoch [3860], val_loss: 26831542.0000\n", "Epoch [3880], val_loss: 28497386.0000\n", "Epoch [3900], val_loss: 26710258.0000\n", "Epoch [3920], val_loss: 29824078.0000\n", "Epoch [3940], val_loss: 27479962.0000\n", "Epoch [3960], val_loss: 31113100.0000\n", "Epoch [3980], val_loss: 27852788.0000\n", "Epoch [4000], val_loss: 26394870.0000\n", "Epoch [4020], val_loss: 26969212.0000\n", "Epoch [4040], val_loss: 27164428.0000\n", "Epoch [4060], val_loss: 27763592.0000\n", "Epoch [4080], val_loss: 26952284.0000\n", "Epoch [4100], val_loss: 29139960.0000\n", "Epoch [4120], val_loss: 28835592.0000\n", "Epoch [4140], val_loss: 27051862.0000\n", "Epoch [4160], val_loss: 26368152.0000\n", "Epoch [4180], val_loss: 34024420.0000\n", "Epoch [4200], val_loss: 26891146.0000\n", "Epoch [4220], val_loss: 26960926.0000\n", "Epoch [4240], val_loss: 26527900.0000\n", "Epoch [4260], val_loss: 36186292.0000\n", "Epoch [4280], val_loss: 26976394.0000\n", "Epoch [4300], val_loss: 27117662.0000\n", "Epoch [4320], val_loss: 32301796.0000\n", "Epoch [4340], val_loss: 27674974.0000\n", "Epoch [4360], val_loss: 26534610.0000\n", "Epoch [4380], val_loss: 29659240.0000\n", "Epoch [4400], val_loss: 29346292.0000\n", "Epoch [4420], val_loss: 27178094.0000\n", "Epoch [4440], val_loss: 28513228.0000\n", "Epoch [4460], val_loss: 26632960.0000\n", "Epoch [4480], val_loss: 26590012.0000\n", "Epoch [4500], val_loss: 27546456.0000\n", "Epoch [4520], val_loss: 26552606.0000\n", "Epoch [4540], val_loss: 29472874.0000\n", "Epoch [4560], val_loss: 27290010.0000\n", "Epoch [4580], val_loss: 31262772.0000\n", "Epoch [4600], val_loss: 26864214.0000\n", "Epoch [4620], val_loss: 28801064.0000\n", "Epoch [4640], val_loss: 26589616.0000\n", "Epoch [4660], val_loss: 28048288.0000\n", "Epoch [4680], val_loss: 26538666.0000\n", "Epoch [4700], val_loss: 26589650.0000\n", "Epoch [4720], val_loss: 26452444.0000\n", "Epoch [4740], val_loss: 39639960.0000\n", "Epoch [4760], val_loss: 35694776.0000\n", "Epoch [4780], val_loss: 26263894.0000\n", "Epoch [4800], val_loss: 30613768.0000\n", "Epoch [4820], val_loss: 50736504.0000\n", "Epoch [4840], val_loss: 26248522.0000\n", "Epoch [4860], val_loss: 26195474.0000\n", "Epoch [4880], val_loss: 26502998.0000\n", "Epoch [4900], val_loss: 27908986.0000\n", "Epoch [4920], val_loss: 26559588.0000\n", "Epoch [4940], val_loss: 26443536.0000\n", "Epoch [4960], val_loss: 26165722.0000\n", "Epoch [4980], val_loss: 27189000.0000\n", "Epoch [5000], val_loss: 33870148.0000\n", "Epoch [5020], val_loss: 31168736.0000\n", "Epoch [5040], val_loss: 26489970.0000\n", "Epoch [5060], val_loss: 27067356.0000\n", "Epoch [5080], val_loss: 27180412.0000\n", "Epoch [5100], val_loss: 30876172.0000\n", "Epoch [5120], val_loss: 26417970.0000\n", "Epoch [5140], val_loss: 33110758.0000\n", "Epoch [5160], val_loss: 26423690.0000\n", "Epoch [5180], val_loss: 26138586.0000\n", "Epoch [5200], val_loss: 36474776.0000\n", "Epoch [5220], val_loss: 26195428.0000\n", "Epoch [5240], val_loss: 26392084.0000\n", "Epoch [5260], val_loss: 26673212.0000\n", "Epoch [5280], val_loss: 41234068.0000\n", "Epoch [5300], val_loss: 27383458.0000\n", "Epoch [5320], val_loss: 26764614.0000\n", "Epoch [5340], val_loss: 26403948.0000\n", "Epoch [5360], val_loss: 26986844.0000\n", "Epoch [5380], val_loss: 27189208.0000\n", "Epoch [5400], val_loss: 41013704.0000\n", "Epoch [5420], val_loss: 30186664.0000\n", "Epoch [5440], val_loss: 27816180.0000\n", "Epoch [5460], val_loss: 32398290.0000\n", "Epoch [5480], val_loss: 27483012.0000\n", "Epoch [5500], val_loss: 26144412.0000\n", "Epoch [5520], val_loss: 48756884.0000\n", "Epoch [5540], val_loss: 28219204.0000\n", "Epoch [5560], val_loss: 27103748.0000\n", "Epoch [5580], val_loss: 33723756.0000\n", "Epoch [5600], val_loss: 27517562.0000\n", "Epoch [5620], val_loss: 27573326.0000\n", "Epoch [5640], val_loss: 26341308.0000\n", "Epoch [5660], val_loss: 26552412.0000\n", "Epoch [5680], val_loss: 26506202.0000\n", "Epoch [5700], val_loss: 27670234.0000\n", "Epoch [5720], val_loss: 39455292.0000\n", "Epoch [5740], val_loss: 30490704.0000\n", "Epoch [5760], val_loss: 54121276.0000\n", "Epoch [5780], val_loss: 26993184.0000\n", "Epoch [5800], val_loss: 30700140.0000\n", "Epoch [5820], val_loss: 26270098.0000\n", "Epoch [5840], val_loss: 26164192.0000\n", "Epoch [5860], val_loss: 27293360.0000\n", "Epoch [5880], val_loss: 26334424.0000\n", "Epoch [5900], val_loss: 43205312.0000\n", "Epoch [5920], val_loss: 57115968.0000\n", "Epoch [5940], val_loss: 29759346.0000\n", "Epoch [5960], val_loss: 51684388.0000\n", "Epoch [5980], val_loss: 30354376.0000\n", "Epoch [6000], val_loss: 26035152.0000\n", "Epoch [6020], val_loss: 26472292.0000\n", "Epoch [6040], val_loss: 27602940.0000\n", "Epoch [6060], val_loss: 27752860.0000\n", "Epoch [6080], val_loss: 27678420.0000\n", "Epoch [6100], val_loss: 26446462.0000\n", "Epoch [6120], val_loss: 26352302.0000\n", "Epoch [6140], val_loss: 29980562.0000\n", "Epoch [6160], val_loss: 38653688.0000\n", "Epoch [6180], val_loss: 28725822.0000\n", "Epoch [6200], val_loss: 26728504.0000\n", "Epoch [6220], val_loss: 26146246.0000\n", "Epoch [6240], val_loss: 26096584.0000\n", "Epoch [6260], val_loss: 28021512.0000\n", "Epoch [6280], val_loss: 26250126.0000\n", "Epoch [6300], val_loss: 27599004.0000\n", "Epoch [6320], val_loss: 27204412.0000\n", "Epoch [6340], val_loss: 26664986.0000\n", "Epoch [6360], val_loss: 26217128.0000\n", "Epoch [6380], val_loss: 28337450.0000\n", "Epoch [6400], val_loss: 27791164.0000\n", "Epoch [6420], val_loss: 27209840.0000\n", "Epoch [6440], val_loss: 30604052.0000\n", "Epoch [6460], val_loss: 45059008.0000\n", "Epoch [6480], val_loss: 28812024.0000\n", "Epoch [6500], val_loss: 26232074.0000\n", "Epoch [6520], val_loss: 54940640.0000\n", "Epoch [6540], val_loss: 26432386.0000\n", "Epoch [6560], val_loss: 26907092.0000\n", "Epoch [6580], val_loss: 26283238.0000\n", "Epoch [6600], val_loss: 26627810.0000\n", "Epoch [6620], val_loss: 28626350.0000\n", "Epoch [6640], val_loss: 26113476.0000\n", "Epoch [6660], val_loss: 29090508.0000\n", "Epoch [6680], val_loss: 26985304.0000\n", "Epoch [6700], val_loss: 26510896.0000\n", "Epoch [6720], val_loss: 26601290.0000\n", "Epoch [6740], val_loss: 27253242.0000\n", "Epoch [6760], val_loss: 26543852.0000\n", "Epoch [6780], val_loss: 26283878.0000\n", "Epoch [6800], val_loss: 27779278.0000\n", "Epoch [6820], val_loss: 26109718.0000\n", "Epoch [6840], val_loss: 33908476.0000\n", "Epoch [6860], val_loss: 27846566.0000\n", "Epoch [6880], val_loss: 26294484.0000\n", "Epoch [6900], val_loss: 29160274.0000\n", "Epoch [6920], val_loss: 28064294.0000\n", "Epoch [6940], val_loss: 30518040.0000\n", "Epoch [6960], val_loss: 26317688.0000\n", "Epoch [6980], val_loss: 27026030.0000\n", "Epoch [7000], val_loss: 26906778.0000\n", "Epoch [7020], val_loss: 38347092.0000\n", "Epoch [7040], val_loss: 34485448.0000\n", "Epoch [7060], val_loss: 26332508.0000\n", "Epoch [7080], val_loss: 26755154.0000\n", "Epoch [7100], val_loss: 26786404.0000\n", "Epoch [7120], val_loss: 26394532.0000\n", "Epoch [7140], val_loss: 27916304.0000\n", "Epoch [7160], val_loss: 28851278.0000\n", "Epoch [7180], val_loss: 26148536.0000\n", "Epoch [7200], val_loss: 27579640.0000\n", "Epoch [7220], val_loss: 27590690.0000\n", "Epoch [7240], val_loss: 27065056.0000\n", "Epoch [7260], val_loss: 29583038.0000\n", "Epoch [7280], val_loss: 54206400.0000\n", "Epoch [7300], val_loss: 26311404.0000\n", "Epoch [7320], val_loss: 29641812.0000\n", "Epoch [7340], val_loss: 37796804.0000\n", "Epoch [7360], val_loss: 26807014.0000\n", "Epoch [7380], val_loss: 26431094.0000\n", "Epoch [7400], val_loss: 26575882.0000\n", "Epoch [7420], val_loss: 27526032.0000\n", "Epoch [7440], val_loss: 27529702.0000\n", "Epoch [7460], val_loss: 35243304.0000\n", "Epoch [7480], val_loss: 42676332.0000\n", "Epoch [7500], val_loss: 27546424.0000\n", "Epoch [7520], val_loss: 26708294.0000\n", "Epoch [7540], val_loss: 26599786.0000\n", "Epoch [7560], val_loss: 26310342.0000\n", "Epoch [7580], val_loss: 26121890.0000\n", "Epoch [7600], val_loss: 26296230.0000\n", "Epoch [7620], val_loss: 27314786.0000\n", "Epoch [7640], val_loss: 26196372.0000\n", "Epoch [7660], val_loss: 28947744.0000\n", "Epoch [7680], val_loss: 26489768.0000\n", "Epoch [7700], val_loss: 27016458.0000\n", "Epoch [7720], val_loss: 26116424.0000\n", "Epoch [7740], val_loss: 27979756.0000\n", "Epoch [7760], val_loss: 26094520.0000\n", "Epoch [7780], val_loss: 38041900.0000\n", "Epoch [7800], val_loss: 26238288.0000\n", "Epoch [7820], val_loss: 27934412.0000\n", "Epoch [7840], val_loss: 28847426.0000\n", "Epoch [7860], val_loss: 26975174.0000\n", "Epoch [7880], val_loss: 26547094.0000\n", "Epoch [7900], val_loss: 27385842.0000\n", "Epoch [7920], val_loss: 26489190.0000\n", "Epoch [7940], val_loss: 26522588.0000\n", "Epoch [7960], val_loss: 26065774.0000\n", "Epoch [7980], val_loss: 32078404.0000\n", "Epoch [8000], val_loss: 25953428.0000\n", "Epoch [8020], val_loss: 28262582.0000\n", "Epoch [8040], val_loss: 28842818.0000\n", "Epoch [8060], val_loss: 26287606.0000\n", "Epoch [8080], val_loss: 32605686.0000\n", "Epoch [8100], val_loss: 26175814.0000\n", "Epoch [8120], val_loss: 27286012.0000\n", "Epoch [8140], val_loss: 28166984.0000\n", "Epoch [8160], val_loss: 25913264.0000\n", "Epoch [8180], val_loss: 27127116.0000\n", "Epoch [8200], val_loss: 26090510.0000\n", "Epoch [8220], val_loss: 36646640.0000\n", "Epoch [8240], val_loss: 27088160.0000\n", "Epoch [8260], val_loss: 32151240.0000\n", "Epoch [8280], val_loss: 30631642.0000\n", "Epoch [8300], val_loss: 35479120.0000\n", "Epoch [8320], val_loss: 25988624.0000\n", "Epoch [8340], val_loss: 28733290.0000\n", "Epoch [8360], val_loss: 26177942.0000\n", "Epoch [8380], val_loss: 26082440.0000\n", "Epoch [8400], val_loss: 30549014.0000\n", "Epoch [8420], val_loss: 29107590.0000\n", "Epoch [8440], val_loss: 29131982.0000\n", "Epoch [8460], val_loss: 26324936.0000\n", "Epoch [8480], val_loss: 26130094.0000\n", "Epoch [8500], val_loss: 26023298.0000\n", "Epoch [8520], val_loss: 29371588.0000\n", "Epoch [8540], val_loss: 30659244.0000\n", "Epoch [8560], val_loss: 30639434.0000\n", "Epoch [8580], val_loss: 26071830.0000\n", "Epoch [8600], val_loss: 34919032.0000\n", "Epoch [8620], val_loss: 30819438.0000\n", "Epoch [8640], val_loss: 31420320.0000\n", "Epoch [8660], val_loss: 26974830.0000\n", "Epoch [8680], val_loss: 26035554.0000\n", "Epoch [8700], val_loss: 26723300.0000\n", "Epoch [8720], val_loss: 25952640.0000\n", "Epoch [8740], val_loss: 26053014.0000\n", "Epoch [8760], val_loss: 30058356.0000\n", "Epoch [8780], val_loss: 41747188.0000\n", "Epoch [8800], val_loss: 30439292.0000\n", "Epoch [8820], val_loss: 28301454.0000\n", "Epoch [8840], val_loss: 28145524.0000\n", "Epoch [8860], val_loss: 27618348.0000\n", "Epoch [8880], val_loss: 27107292.0000\n", "Epoch [8900], val_loss: 28053064.0000\n", "Epoch [8920], val_loss: 26263772.0000\n", "Epoch [8940], val_loss: 27154032.0000\n", "Epoch [8960], val_loss: 27594012.0000\n", "Epoch [8980], val_loss: 27697024.0000\n", "Epoch [9000], val_loss: 35786860.0000\n", "Epoch [9020], val_loss: 26142708.0000\n", "Epoch [9040], val_loss: 26131764.0000\n", "Epoch [9060], val_loss: 29506536.0000\n", "Epoch [9080], val_loss: 56787056.0000\n", "Epoch [9100], val_loss: 32931174.0000\n", "Epoch [9120], val_loss: 26253392.0000\n", "Epoch [9140], val_loss: 26933790.0000\n", "Epoch [9160], val_loss: 26657372.0000\n", "Epoch [9180], val_loss: 29299986.0000\n", "Epoch [9200], val_loss: 27382854.0000\n", "Epoch [9220], val_loss: 28263090.0000\n", "Epoch [9240], val_loss: 28480446.0000\n", "Epoch [9260], val_loss: 26323848.0000\n", "Epoch [9280], val_loss: 37487876.0000\n", "Epoch [9300], val_loss: 25986286.0000\n", "Epoch [9320], val_loss: 29964072.0000\n", "Epoch [9340], val_loss: 27582836.0000\n", "Epoch [9360], val_loss: 26239558.0000\n", "Epoch [9380], val_loss: 28465190.0000\n", "Epoch [9400], val_loss: 29893258.0000\n", "Epoch [9420], val_loss: 28892412.0000\n", "Epoch [9440], val_loss: 30511772.0000\n", "Epoch [9460], val_loss: 29883790.0000\n", "Epoch [9480], val_loss: 42902640.0000\n", "Epoch [9500], val_loss: 26463356.0000\n", "Epoch [9520], val_loss: 25924128.0000\n", "Epoch [9540], val_loss: 26078712.0000\n", "Epoch [9560], val_loss: 27006126.0000\n", "Epoch [9580], val_loss: 29164986.0000\n", "Epoch [9600], val_loss: 26038992.0000\n", "Epoch [9620], val_loss: 26090922.0000\n", "Epoch [9640], val_loss: 30187938.0000\n", "Epoch [9660], val_loss: 26397994.0000\n", "Epoch [9680], val_loss: 30207228.0000\n", "Epoch [9700], val_loss: 27543686.0000\n", "Epoch [9720], val_loss: 31086240.0000\n", "Epoch [9740], val_loss: 27456842.0000\n", "Epoch [9760], val_loss: 27297004.0000\n", "Epoch [9780], val_loss: 26095260.0000\n", "Epoch [9800], val_loss: 26198528.0000\n", "Epoch [9820], val_loss: 26248488.0000\n", "Epoch [9840], val_loss: 26010782.0000\n", "Epoch [9860], val_loss: 27319624.0000\n", "Epoch [9880], val_loss: 26136948.0000\n", "Epoch [9900], val_loss: 26223722.0000\n", "Epoch [9920], val_loss: 26039896.0000\n", "Epoch [9940], val_loss: 37869708.0000\n", "Epoch [9960], val_loss: 28759026.0000\n", "Epoch [9980], val_loss: 26844712.0000\n", "Epoch [10000], val_loss: 25943460.0000\n"]}], "source": ["epochs = 10000\n", "lr = 1e-4\n", "history3 = fit(epochs, lr, model, train_loader, val_loader)"]}, {"cell_type": "code", "execution_count": 203, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 516596, "status": "ok", "timestamp": 1607756191217, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "OOrmlch8RB9G", "outputId": "a665217b-723b-479c-9385-b77ee606a381"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch [20], val_loss: 27141698.0000\n", "Epoch [40], val_loss: 26445914.0000\n", "Epoch [60], val_loss: 26845712.0000\n", "Epoch [80], val_loss: 26598638.0000\n", "Epoch [100], val_loss: 27251600.0000\n", "Epoch [120], val_loss: 26463882.0000\n", "Epoch [140], val_loss: 26937846.0000\n", "Epoch [160], val_loss: 27119578.0000\n", "Epoch [180], val_loss: 27087886.0000\n", "Epoch [200], val_loss: 26658870.0000\n", "Epoch [220], val_loss: 26564906.0000\n", "Epoch [240], val_loss: 26734788.0000\n", "Epoch [260], val_loss: 26735748.0000\n", "Epoch [280], val_loss: 27575368.0000\n", "Epoch [300], val_loss: 26720126.0000\n", "Epoch [320], val_loss: 26615154.0000\n", "Epoch [340], val_loss: 26904428.0000\n", "Epoch [360], val_loss: 27308214.0000\n", "Epoch [380], val_loss: 26977820.0000\n", "Epoch [400], val_loss: 26365904.0000\n", "Epoch [420], val_loss: 27287402.0000\n", "Epoch [440], val_loss: 27629858.0000\n", "Epoch [460], val_loss: 27056784.0000\n", "Epoch [480], val_loss: 26989174.0000\n", "Epoch [500], val_loss: 26480512.0000\n", "Epoch [520], val_loss: 26469238.0000\n", "Epoch [540], val_loss: 26856520.0000\n", "Epoch [560], val_loss: 26688324.0000\n", "Epoch [580], val_loss: 26898958.0000\n", "Epoch [600], val_loss: 26685912.0000\n", "Epoch [620], val_loss: 27090124.0000\n", "Epoch [640], val_loss: 27082302.0000\n", "Epoch [660], val_loss: 26736480.0000\n", "Epoch [680], val_loss: 26964074.0000\n", "Epoch [700], val_loss: 26743022.0000\n", "Epoch [720], val_loss: 26850552.0000\n", "Epoch [740], val_loss: 26491538.0000\n", "Epoch [760], val_loss: 26541056.0000\n", "Epoch [780], val_loss: 26984190.0000\n", "Epoch [800], val_loss: 26375518.0000\n", "Epoch [820], val_loss: 27237726.0000\n", "Epoch [840], val_loss: 26594448.0000\n", "Epoch [860], val_loss: 26885352.0000\n", "Epoch [880], val_loss: 26749320.0000\n", "Epoch [900], val_loss: 26912012.0000\n", "Epoch [920], val_loss: 26859350.0000\n", "Epoch [940], val_loss: 26673776.0000\n", "Epoch [960], val_loss: 26834706.0000\n", "Epoch [980], val_loss: 26724858.0000\n", "Epoch [1000], val_loss: 26848540.0000\n", "Epoch [1020], val_loss: 26900810.0000\n", "Epoch [1040], val_loss: 26453588.0000\n", "Epoch [1060], val_loss: 26661646.0000\n", "Epoch [1080], val_loss: 26782582.0000\n", "Epoch [1100], val_loss: 26853542.0000\n", "Epoch [1120], val_loss: 27020784.0000\n", "Epoch [1140], val_loss: 27233532.0000\n", "Epoch [1160], val_loss: 26845382.0000\n", "Epoch [1180], val_loss: 26755670.0000\n", "Epoch [1200], val_loss: 26934934.0000\n", "Epoch [1220], val_loss: 27563190.0000\n", "Epoch [1240], val_loss: 26692060.0000\n", "Epoch [1260], val_loss: 26977942.0000\n", "Epoch [1280], val_loss: 26571372.0000\n", "Epoch [1300], val_loss: 26986874.0000\n", "Epoch [1320], val_loss: 27702504.0000\n", "Epoch [1340], val_loss: 27242458.0000\n", "Epoch [1360], val_loss: 27349742.0000\n", "Epoch [1380], val_loss: 26543132.0000\n", "Epoch [1400], val_loss: 27816862.0000\n", "Epoch [1420], val_loss: 26613204.0000\n", "Epoch [1440], val_loss: 26740142.0000\n", "Epoch [1460], val_loss: 26566700.0000\n", "Epoch [1480], val_loss: 27228448.0000\n", "Epoch [1500], val_loss: 27262462.0000\n", "Epoch [1520], val_loss: 27138860.0000\n", "Epoch [1540], val_loss: 27097632.0000\n", "Epoch [1560], val_loss: 26911176.0000\n", "Epoch [1580], val_loss: 27421980.0000\n", "Epoch [1600], val_loss: 27299542.0000\n", "Epoch [1620], val_loss: 27169786.0000\n", "Epoch [1640], val_loss: 26953462.0000\n", "Epoch [1660], val_loss: 26721740.0000\n", "Epoch [1680], val_loss: 26890528.0000\n", "Epoch [1700], val_loss: 26616302.0000\n", "Epoch [1720], val_loss: 26495830.0000\n", "Epoch [1740], val_loss: 26848068.0000\n", "Epoch [1760], val_loss: 26452866.0000\n", "Epoch [1780], val_loss: 26828634.0000\n", "Epoch [1800], val_loss: 26928306.0000\n", "Epoch [1820], val_loss: 27126866.0000\n", "Epoch [1840], val_loss: 26415588.0000\n", "Epoch [1860], val_loss: 26608116.0000\n", "Epoch [1880], val_loss: 26930680.0000\n", "Epoch [1900], val_loss: 27087884.0000\n", "Epoch [1920], val_loss: 26537686.0000\n", "Epoch [1940], val_loss: 27184574.0000\n", "Epoch [1960], val_loss: 26474024.0000\n", "Epoch [1980], val_loss: 26647310.0000\n", "Epoch [2000], val_loss: 27234308.0000\n", "Epoch [2020], val_loss: 26443370.0000\n", "Epoch [2040], val_loss: 26520296.0000\n", "Epoch [2060], val_loss: 26726238.0000\n", "Epoch [2080], val_loss: 26878664.0000\n", "Epoch [2100], val_loss: 26855322.0000\n", "Epoch [2120], val_loss: 27554008.0000\n", "Epoch [2140], val_loss: 26472446.0000\n", "Epoch [2160], val_loss: 26724656.0000\n", "Epoch [2180], val_loss: 26567108.0000\n", "Epoch [2200], val_loss: 26692870.0000\n", "Epoch [2220], val_loss: 27588196.0000\n", "Epoch [2240], val_loss: 26678638.0000\n", "Epoch [2260], val_loss: 26849204.0000\n", "Epoch [2280], val_loss: 26500504.0000\n", "Epoch [2300], val_loss: 26391772.0000\n", "Epoch [2320], val_loss: 26833754.0000\n", "Epoch [2340], val_loss: 26465356.0000\n", "Epoch [2360], val_loss: 26430440.0000\n", "Epoch [2380], val_loss: 26884300.0000\n", "Epoch [2400], val_loss: 26487450.0000\n", "Epoch [2420], val_loss: 26747758.0000\n", "Epoch [2440], val_loss: 26766694.0000\n", "Epoch [2460], val_loss: 26534128.0000\n", "Epoch [2480], val_loss: 26702986.0000\n", "Epoch [2500], val_loss: 26755506.0000\n", "Epoch [2520], val_loss: 27022808.0000\n", "Epoch [2540], val_loss: 27092772.0000\n", "Epoch [2560], val_loss: 27137504.0000\n", "Epoch [2580], val_loss: 26846042.0000\n", "Epoch [2600], val_loss: 26794886.0000\n", "Epoch [2620], val_loss: 27165778.0000\n", "Epoch [2640], val_loss: 27026506.0000\n", "Epoch [2660], val_loss: 26830556.0000\n", "Epoch [2680], val_loss: 26573874.0000\n", "Epoch [2700], val_loss: 26721500.0000\n", "Epoch [2720], val_loss: 26495854.0000\n", "Epoch [2740], val_loss: 26951482.0000\n", "Epoch [2760], val_loss: 27006582.0000\n", "Epoch [2780], val_loss: 26695950.0000\n", "Epoch [2800], val_loss: 26446312.0000\n", "Epoch [2820], val_loss: 27064054.0000\n", "Epoch [2840], val_loss: 26902164.0000\n", "Epoch [2860], val_loss: 26367804.0000\n", "Epoch [2880], val_loss: 27177358.0000\n", "Epoch [2900], val_loss: 26795790.0000\n", "Epoch [2920], val_loss: 26601814.0000\n", "Epoch [2940], val_loss: 28026438.0000\n", "Epoch [2960], val_loss: 26836060.0000\n", "Epoch [2980], val_loss: 27038726.0000\n", "Epoch [3000], val_loss: 26680466.0000\n", "Epoch [3020], val_loss: 27550516.0000\n", "Epoch [3040], val_loss: 26868078.0000\n", "Epoch [3060], val_loss: 26824294.0000\n", "Epoch [3080], val_loss: 26954546.0000\n", "Epoch [3100], val_loss: 26871682.0000\n", "Epoch [3120], val_loss: 26729750.0000\n", "Epoch [3140], val_loss: 26857434.0000\n", "Epoch [3160], val_loss: 27014834.0000\n", "Epoch [3180], val_loss: 26673738.0000\n", "Epoch [3200], val_loss: 26709766.0000\n", "Epoch [3220], val_loss: 27293916.0000\n", "Epoch [3240], val_loss: 26917780.0000\n", "Epoch [3260], val_loss: 26564164.0000\n", "Epoch [3280], val_loss: 27267794.0000\n", "Epoch [3300], val_loss: 26788726.0000\n", "Epoch [3320], val_loss: 27290838.0000\n", "Epoch [3340], val_loss: 26912110.0000\n", "Epoch [3360], val_loss: 26547524.0000\n", "Epoch [3380], val_loss: 26718476.0000\n", "Epoch [3400], val_loss: 27048610.0000\n", "Epoch [3420], val_loss: 26917954.0000\n", "Epoch [3440], val_loss: 26855906.0000\n", "Epoch [3460], val_loss: 26484878.0000\n", "Epoch [3480], val_loss: 26671904.0000\n", "Epoch [3500], val_loss: 26889220.0000\n", "Epoch [3520], val_loss: 26337544.0000\n", "Epoch [3540], val_loss: 26381286.0000\n", "Epoch [3560], val_loss: 26698544.0000\n", "Epoch [3580], val_loss: 26672280.0000\n", "Epoch [3600], val_loss: 26605560.0000\n", "Epoch [3620], val_loss: 27459454.0000\n", "Epoch [3640], val_loss: 26717078.0000\n", "Epoch [3660], val_loss: 26917518.0000\n", "Epoch [3680], val_loss: 26864522.0000\n", "Epoch [3700], val_loss: 26979412.0000\n", "Epoch [3720], val_loss: 26716554.0000\n", "Epoch [3740], val_loss: 27265484.0000\n", "Epoch [3760], val_loss: 27186720.0000\n", "Epoch [3780], val_loss: 27117338.0000\n", "Epoch [3800], val_loss: 26621088.0000\n", "Epoch [3820], val_loss: 27411062.0000\n", "Epoch [3840], val_loss: 26707880.0000\n", "Epoch [3860], val_loss: 26604886.0000\n", "Epoch [3880], val_loss: 26729456.0000\n", "Epoch [3900], val_loss: 26865318.0000\n", "Epoch [3920], val_loss: 26712888.0000\n", "Epoch [3940], val_loss: 26755146.0000\n", "Epoch [3960], val_loss: 26592360.0000\n", "Epoch [3980], val_loss: 27215182.0000\n", "Epoch [4000], val_loss: 26965470.0000\n", "Epoch [4020], val_loss: 26537324.0000\n", "Epoch [4040], val_loss: 26453452.0000\n", "Epoch [4060], val_loss: 27656900.0000\n", "Epoch [4080], val_loss: 26713160.0000\n", "Epoch [4100], val_loss: 27221754.0000\n", "Epoch [4120], val_loss: 27010032.0000\n", "Epoch [4140], val_loss: 26736602.0000\n", "Epoch [4160], val_loss: 27337260.0000\n", "Epoch [4180], val_loss: 26665478.0000\n", "Epoch [4200], val_loss: 26989004.0000\n", "Epoch [4220], val_loss: 27327308.0000\n", "Epoch [4240], val_loss: 27280670.0000\n", "Epoch [4260], val_loss: 27373970.0000\n", "Epoch [4280], val_loss: 26654018.0000\n", "Epoch [4300], val_loss: 27388392.0000\n", "Epoch [4320], val_loss: 27024716.0000\n", "Epoch [4340], val_loss: 26692516.0000\n", "Epoch [4360], val_loss: 27212790.0000\n", "Epoch [4380], val_loss: 26498928.0000\n", "Epoch [4400], val_loss: 27452422.0000\n", "Epoch [4420], val_loss: 27257614.0000\n", "Epoch [4440], val_loss: 26438680.0000\n", "Epoch [4460], val_loss: 26739190.0000\n", "Epoch [4480], val_loss: 26762026.0000\n", "Epoch [4500], val_loss: 26513590.0000\n", "Epoch [4520], val_loss: 26811566.0000\n", "Epoch [4540], val_loss: 26503422.0000\n", "Epoch [4560], val_loss: 26735970.0000\n", "Epoch [4580], val_loss: 27523134.0000\n", "Epoch [4600], val_loss: 26568200.0000\n", "Epoch [4620], val_loss: 26824328.0000\n", "Epoch [4640], val_loss: 26868558.0000\n", "Epoch [4660], val_loss: 26299030.0000\n", "Epoch [4680], val_loss: 26926464.0000\n", "Epoch [4700], val_loss: 26716334.0000\n", "Epoch [4720], val_loss: 26734358.0000\n", "Epoch [4740], val_loss: 27016866.0000\n", "Epoch [4760], val_loss: 26503458.0000\n", "Epoch [4780], val_loss: 26939428.0000\n", "Epoch [4800], val_loss: 26516414.0000\n", "Epoch [4820], val_loss: 27379708.0000\n", "Epoch [4840], val_loss: 26907652.0000\n", "Epoch [4860], val_loss: 27020138.0000\n", "Epoch [4880], val_loss: 26435390.0000\n", "Epoch [4900], val_loss: 27143272.0000\n", "Epoch [4920], val_loss: 27064324.0000\n", "Epoch [4940], val_loss: 28145802.0000\n", "Epoch [4960], val_loss: 26979918.0000\n", "Epoch [4980], val_loss: 26793162.0000\n", "Epoch [5000], val_loss: 26976474.0000\n", "Epoch [5020], val_loss: 26844518.0000\n", "Epoch [5040], val_loss: 26485086.0000\n", "Epoch [5060], val_loss: 26609832.0000\n", "Epoch [5080], val_loss: 26472970.0000\n", "Epoch [5100], val_loss: 26549476.0000\n", "Epoch [5120], val_loss: 26663276.0000\n", "Epoch [5140], val_loss: 26464428.0000\n", "Epoch [5160], val_loss: 27053162.0000\n", "Epoch [5180], val_loss: 26677928.0000\n", "Epoch [5200], val_loss: 26657582.0000\n", "Epoch [5220], val_loss: 26866126.0000\n", "Epoch [5240], val_loss: 26684806.0000\n", "Epoch [5260], val_loss: 27035344.0000\n", "Epoch [5280], val_loss: 26522750.0000\n", "Epoch [5300], val_loss: 26678564.0000\n", "Epoch [5320], val_loss: 26793526.0000\n", "Epoch [5340], val_loss: 27288572.0000\n", "Epoch [5360], val_loss: 26837122.0000\n", "Epoch [5380], val_loss: 26521766.0000\n", "Epoch [5400], val_loss: 26832326.0000\n", "Epoch [5420], val_loss: 26912666.0000\n", "Epoch [5440], val_loss: 26758616.0000\n", "Epoch [5460], val_loss: 26613066.0000\n", "Epoch [5480], val_loss: 26758110.0000\n", "Epoch [5500], val_loss: 26739886.0000\n", "Epoch [5520], val_loss: 26640530.0000\n", "Epoch [5540], val_loss: 26850868.0000\n", "Epoch [5560], val_loss: 26683372.0000\n", "Epoch [5580], val_loss: 26730742.0000\n", "Epoch [5600], val_loss: 26695594.0000\n", "Epoch [5620], val_loss: 26867228.0000\n", "Epoch [5640], val_loss: 26439642.0000\n", "Epoch [5660], val_loss: 26671436.0000\n", "Epoch [5680], val_loss: 26615256.0000\n", "Epoch [5700], val_loss: 27944032.0000\n", "Epoch [5720], val_loss: 26849116.0000\n", "Epoch [5740], val_loss: 26443160.0000\n", "Epoch [5760], val_loss: 26975310.0000\n", "Epoch [5780], val_loss: 26823854.0000\n", "Epoch [5800], val_loss: 26361908.0000\n", "Epoch [5820], val_loss: 26897564.0000\n", "Epoch [5840], val_loss: 27030096.0000\n", "Epoch [5860], val_loss: 26894770.0000\n", "Epoch [5880], val_loss: 27646514.0000\n", "Epoch [5900], val_loss: 26545366.0000\n", "Epoch [5920], val_loss: 27331700.0000\n", "Epoch [5940], val_loss: 27251220.0000\n", "Epoch [5960], val_loss: 26538940.0000\n", "Epoch [5980], val_loss: 26980434.0000\n", "Epoch [6000], val_loss: 26787902.0000\n", "Epoch [6020], val_loss: 27078152.0000\n", "Epoch [6040], val_loss: 26461842.0000\n", "Epoch [6060], val_loss: 26203892.0000\n", "Epoch [6080], val_loss: 27082246.0000\n", "Epoch [6100], val_loss: 26780184.0000\n", "Epoch [6120], val_loss: 26981450.0000\n", "Epoch [6140], val_loss: 26462992.0000\n", "Epoch [6160], val_loss: 26761130.0000\n", "Epoch [6180], val_loss: 26801324.0000\n", "Epoch [6200], val_loss: 27182706.0000\n", "Epoch [6220], val_loss: 26755108.0000\n", "Epoch [6240], val_loss: 27009828.0000\n", "Epoch [6260], val_loss: 26897616.0000\n", "Epoch [6280], val_loss: 26848024.0000\n", "Epoch [6300], val_loss: 26328202.0000\n", "Epoch [6320], val_loss: 26668876.0000\n", "Epoch [6340], val_loss: 26639174.0000\n", "Epoch [6360], val_loss: 26862506.0000\n", "Epoch [6380], val_loss: 26394760.0000\n", "Epoch [6400], val_loss: 26921400.0000\n", "Epoch [6420], val_loss: 26332656.0000\n", "Epoch [6440], val_loss: 27055160.0000\n", "Epoch [6460], val_loss: 26847028.0000\n", "Epoch [6480], val_loss: 26903784.0000\n", "Epoch [6500], val_loss: 26938282.0000\n", "Epoch [6520], val_loss: 26909820.0000\n", "Epoch [6540], val_loss: 27015054.0000\n", "Epoch [6560], val_loss: 26675252.0000\n", "Epoch [6580], val_loss: 26222200.0000\n", "Epoch [6600], val_loss: 27062780.0000\n", "Epoch [6620], val_loss: 26762446.0000\n", "Epoch [6640], val_loss: 26880618.0000\n", "Epoch [6660], val_loss: 27121610.0000\n", "Epoch [6680], val_loss: 26695094.0000\n", "Epoch [6700], val_loss: 27084154.0000\n", "Epoch [6720], val_loss: 26962342.0000\n", "Epoch [6740], val_loss: 26492918.0000\n", "Epoch [6760], val_loss: 26816360.0000\n", "Epoch [6780], val_loss: 27287330.0000\n", "Epoch [6800], val_loss: 26488032.0000\n", "Epoch [6820], val_loss: 26673388.0000\n", "Epoch [6840], val_loss: 26725458.0000\n", "Epoch [6860], val_loss: 26963036.0000\n", "Epoch [6880], val_loss: 26898164.0000\n", "Epoch [6900], val_loss: 26949580.0000\n", "Epoch [6920], val_loss: 27015624.0000\n", "Epoch [6940], val_loss: 26778284.0000\n", "Epoch [6960], val_loss: 27687166.0000\n", "Epoch [6980], val_loss: 26878204.0000\n", "Epoch [7000], val_loss: 26712792.0000\n", "Epoch [7020], val_loss: 27198206.0000\n", "Epoch [7040], val_loss: 27152028.0000\n", "Epoch [7060], val_loss: 26958698.0000\n", "Epoch [7080], val_loss: 26826488.0000\n", "Epoch [7100], val_loss: 26689512.0000\n", "Epoch [7120], val_loss: 26591614.0000\n", "Epoch [7140], val_loss: 26908586.0000\n", "Epoch [7160], val_loss: 26373262.0000\n", "Epoch [7180], val_loss: 26608814.0000\n", "Epoch [7200], val_loss: 26721280.0000\n", "Epoch [7220], val_loss: 27110634.0000\n", "Epoch [7240], val_loss: 28267850.0000\n", "Epoch [7260], val_loss: 26940076.0000\n", "Epoch [7280], val_loss: 27071772.0000\n", "Epoch [7300], val_loss: 26370602.0000\n", "Epoch [7320], val_loss: 26714112.0000\n", "Epoch [7340], val_loss: 26894756.0000\n", "Epoch [7360], val_loss: 26710502.0000\n", "Epoch [7380], val_loss: 26723536.0000\n", "Epoch [7400], val_loss: 26560816.0000\n", "Epoch [7420], val_loss: 26827418.0000\n", "Epoch [7440], val_loss: 27369600.0000\n", "Epoch [7460], val_loss: 27166406.0000\n", "Epoch [7480], val_loss: 26679124.0000\n", "Epoch [7500], val_loss: 26756004.0000\n", "Epoch [7520], val_loss: 26679650.0000\n", "Epoch [7540], val_loss: 27164086.0000\n", "Epoch [7560], val_loss: 26682114.0000\n", "Epoch [7580], val_loss: 27019232.0000\n", "Epoch [7600], val_loss: 27045620.0000\n", "Epoch [7620], val_loss: 26578284.0000\n", "Epoch [7640], val_loss: 26557940.0000\n", "Epoch [7660], val_loss: 26960608.0000\n", "Epoch [7680], val_loss: 27059200.0000\n", "Epoch [7700], val_loss: 26624224.0000\n", "Epoch [7720], val_loss: 26896510.0000\n", "Epoch [7740], val_loss: 27082784.0000\n", "Epoch [7760], val_loss: 26645004.0000\n", "Epoch [7780], val_loss: 26815228.0000\n", "Epoch [7800], val_loss: 27341660.0000\n", "Epoch [7820], val_loss: 26738844.0000\n", "Epoch [7840], val_loss: 27149600.0000\n", "Epoch [7860], val_loss: 26585856.0000\n", "Epoch [7880], val_loss: 27392866.0000\n", "Epoch [7900], val_loss: 26855736.0000\n", "Epoch [7920], val_loss: 27149434.0000\n", "Epoch [7940], val_loss: 27105704.0000\n", "Epoch [7960], val_loss: 26708378.0000\n", "Epoch [7980], val_loss: 26919492.0000\n", "Epoch [8000], val_loss: 26591726.0000\n", "Epoch [8020], val_loss: 26663550.0000\n", "Epoch [8040], val_loss: 26876634.0000\n", "Epoch [8060], val_loss: 27000636.0000\n", "Epoch [8080], val_loss: 26465454.0000\n", "Epoch [8100], val_loss: 27014756.0000\n", "Epoch [8120], val_loss: 26451972.0000\n", "Epoch [8140], val_loss: 27259448.0000\n", "Epoch [8160], val_loss: 26666662.0000\n", "Epoch [8180], val_loss: 26773074.0000\n", "Epoch [8200], val_loss: 27153890.0000\n", "Epoch [8220], val_loss: 26976910.0000\n", "Epoch [8240], val_loss: 27465650.0000\n", "Epoch [8260], val_loss: 26687282.0000\n", "Epoch [8280], val_loss: 26708728.0000\n", "Epoch [8300], val_loss: 27015846.0000\n", "Epoch [8320], val_loss: 26599286.0000\n", "Epoch [8340], val_loss: 27257302.0000\n", "Epoch [8360], val_loss: 26723946.0000\n", "Epoch [8380], val_loss: 26650946.0000\n", "Epoch [8400], val_loss: 26437856.0000\n", "Epoch [8420], val_loss: 27201070.0000\n", "Epoch [8440], val_loss: 28044364.0000\n", "Epoch [8460], val_loss: 26820310.0000\n", "Epoch [8480], val_loss: 26553750.0000\n", "Epoch [8500], val_loss: 27030286.0000\n", "Epoch [8520], val_loss: 26576510.0000\n", "Epoch [8540], val_loss: 26673566.0000\n", "Epoch [8560], val_loss: 26532856.0000\n", "Epoch [8580], val_loss: 26834528.0000\n", "Epoch [8600], val_loss: 27036176.0000\n", "Epoch [8620], val_loss: 26670720.0000\n", "Epoch [8640], val_loss: 26794620.0000\n", "Epoch [8660], val_loss: 26303322.0000\n", "Epoch [8680], val_loss: 26551816.0000\n", "Epoch [8700], val_loss: 27190756.0000\n", "Epoch [8720], val_loss: 26896222.0000\n", "Epoch [8740], val_loss: 26654508.0000\n", "Epoch [8760], val_loss: 26711292.0000\n", "Epoch [8780], val_loss: 26768404.0000\n", "Epoch [8800], val_loss: 27414328.0000\n", "Epoch [8820], val_loss: 26681442.0000\n", "Epoch [8840], val_loss: 26862974.0000\n", "Epoch [8860], val_loss: 26433502.0000\n", "Epoch [8880], val_loss: 27029630.0000\n", "Epoch [8900], val_loss: 26575576.0000\n", "Epoch [8920], val_loss: 27597950.0000\n", "Epoch [8940], val_loss: 26994564.0000\n", "Epoch [8960], val_loss: 27422856.0000\n", "Epoch [8980], val_loss: 26728328.0000\n", "Epoch [9000], val_loss: 27171364.0000\n", "Epoch [9020], val_loss: 26979380.0000\n", "Epoch [9040], val_loss: 27037972.0000\n", "Epoch [9060], val_loss: 26691414.0000\n", "Epoch [9080], val_loss: 26907058.0000\n", "Epoch [9100], val_loss: 26853930.0000\n", "Epoch [9120], val_loss: 26881506.0000\n", "Epoch [9140], val_loss: 27120914.0000\n", "Epoch [9160], val_loss: 26412708.0000\n", "Epoch [9180], val_loss: 26864478.0000\n", "Epoch [9200], val_loss: 26897724.0000\n", "Epoch [9220], val_loss: 26928428.0000\n", "Epoch [9240], val_loss: 26718692.0000\n", "Epoch [9260], val_loss: 26591056.0000\n", "Epoch [9280], val_loss: 26895744.0000\n", "Epoch [9300], val_loss: 26626604.0000\n", "Epoch [9320], val_loss: 26695416.0000\n", "Epoch [9340], val_loss: 26888276.0000\n", "Epoch [9360], val_loss: 26635972.0000\n", "Epoch [9380], val_loss: 27358858.0000\n", "Epoch [9400], val_loss: 26323880.0000\n", "Epoch [9420], val_loss: 26698514.0000\n", "Epoch [9440], val_loss: 26700442.0000\n", "Epoch [9460], val_loss: 27163224.0000\n", "Epoch [9480], val_loss: 26740054.0000\n", "Epoch [9500], val_loss: 26755646.0000\n", "Epoch [9520], val_loss: 27032828.0000\n", "Epoch [9540], val_loss: 26672876.0000\n", "Epoch [9560], val_loss: 27238960.0000\n", "Epoch [9580], val_loss: 26918132.0000\n", "Epoch [9600], val_loss: 26525950.0000\n", "Epoch [9620], val_loss: 26707186.0000\n", "Epoch [9640], val_loss: 26572902.0000\n", "Epoch [9660], val_loss: 26454740.0000\n", "Epoch [9680], val_loss: 27629272.0000\n", "Epoch [9700], val_loss: 27160844.0000\n", "Epoch [9720], val_loss: 26956092.0000\n", "Epoch [9740], val_loss: 27122136.0000\n", "Epoch [9760], val_loss: 26566236.0000\n", "Epoch [9780], val_loss: 26686038.0000\n", "Epoch [9800], val_loss: 26818460.0000\n", "Epoch [9820], val_loss: 27041696.0000\n", "Epoch [9840], val_loss: 27178354.0000\n", "Epoch [9860], val_loss: 27042786.0000\n", "Epoch [9880], val_loss: 26304898.0000\n", "Epoch [9900], val_loss: 26631708.0000\n", "Epoch [9920], val_loss: 26710618.0000\n", "Epoch [9940], val_loss: 26568616.0000\n", "Epoch [9960], val_loss: 26838732.0000\n", "Epoch [9980], val_loss: 27629978.0000\n", "Epoch [10000], val_loss: 26792634.0000\n"]}], "source": ["epochs = 10000\n", "lr = 1e-5\n", "history4 = fit(epochs, lr, model, train_loader, val_loader)"]}, {"cell_type": "code", "execution_count": 204, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 681895, "status": "ok", "timestamp": 1607756356522, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "ZQMKvXNCRB9G", "outputId": "183fa7ae-5867-4f2a-f7de-6a7c68dced29"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch [20], val_loss: 26524632.0000\n", "Epoch [40], val_loss: 26805314.0000\n", "Epoch [60], val_loss: 27037492.0000\n", "Epoch [80], val_loss: 26736470.0000\n", "Epoch [100], val_loss: 26787620.0000\n", "Epoch [120], val_loss: 26820050.0000\n", "Epoch [140], val_loss: 26521192.0000\n", "Epoch [160], val_loss: 26694474.0000\n", "Epoch [180], val_loss: 27069182.0000\n", "Epoch [200], val_loss: 27026488.0000\n", "Epoch [220], val_loss: 26636188.0000\n", "Epoch [240], val_loss: 26623968.0000\n", "Epoch [260], val_loss: 26768768.0000\n", "Epoch [280], val_loss: 26817294.0000\n", "Epoch [300], val_loss: 26834006.0000\n", "Epoch [320], val_loss: 27348368.0000\n", "Epoch [340], val_loss: 26752306.0000\n", "Epoch [360], val_loss: 27484464.0000\n", "Epoch [380], val_loss: 27042514.0000\n", "Epoch [400], val_loss: 26831924.0000\n", "Epoch [420], val_loss: 26780720.0000\n", "Epoch [440], val_loss: 26801964.0000\n", "Epoch [460], val_loss: 26727010.0000\n", "Epoch [480], val_loss: 27132208.0000\n", "Epoch [500], val_loss: 26722114.0000\n", "Epoch [520], val_loss: 26774216.0000\n", "Epoch [540], val_loss: 26721390.0000\n", "Epoch [560], val_loss: 26403634.0000\n", "Epoch [580], val_loss: 26935570.0000\n", "Epoch [600], val_loss: 27549996.0000\n", "Epoch [620], val_loss: 26687924.0000\n", "Epoch [640], val_loss: 27183410.0000\n", "Epoch [660], val_loss: 26927970.0000\n", "Epoch [680], val_loss: 27427670.0000\n", "Epoch [700], val_loss: 27033054.0000\n", "Epoch [720], val_loss: 26370028.0000\n", "Epoch [740], val_loss: 26664780.0000\n", "Epoch [760], val_loss: 26364708.0000\n", "Epoch [780], val_loss: 27077620.0000\n", "Epoch [800], val_loss: 26514526.0000\n", "Epoch [820], val_loss: 26597306.0000\n", "Epoch [840], val_loss: 26577504.0000\n", "Epoch [860], val_loss: 26921970.0000\n", "Epoch [880], val_loss: 26663030.0000\n", "Epoch [900], val_loss: 26867078.0000\n", "Epoch [920], val_loss: 26627026.0000\n", "Epoch [940], val_loss: 26970444.0000\n", "Epoch [960], val_loss: 26705424.0000\n", "Epoch [980], val_loss: 27394694.0000\n", "Epoch [1000], val_loss: 26972998.0000\n", "Epoch [1020], val_loss: 27567426.0000\n", "Epoch [1040], val_loss: 26816770.0000\n", "Epoch [1060], val_loss: 26419330.0000\n", "Epoch [1080], val_loss: 26964880.0000\n", "Epoch [1100], val_loss: 27166070.0000\n", "Epoch [1120], val_loss: 26402224.0000\n", "Epoch [1140], val_loss: 26912630.0000\n", "Epoch [1160], val_loss: 26480028.0000\n", "Epoch [1180], val_loss: 26838584.0000\n", "Epoch [1200], val_loss: 26856906.0000\n", "Epoch [1220], val_loss: 26933524.0000\n", "Epoch [1240], val_loss: 26282960.0000\n", "Epoch [1260], val_loss: 26463968.0000\n", "Epoch [1280], val_loss: 26391546.0000\n", "Epoch [1300], val_loss: 27003978.0000\n", "Epoch [1320], val_loss: 26614688.0000\n", "Epoch [1340], val_loss: 26808068.0000\n", "Epoch [1360], val_loss: 27445172.0000\n", "Epoch [1380], val_loss: 26921562.0000\n", "Epoch [1400], val_loss: 26954812.0000\n", "Epoch [1420], val_loss: 27210292.0000\n", "Epoch [1440], val_loss: 26669910.0000\n", "Epoch [1460], val_loss: 26822790.0000\n", "Epoch [1480], val_loss: 27068844.0000\n", "Epoch [1500], val_loss: 27102238.0000\n", "Epoch [1520], val_loss: 26835890.0000\n", "Epoch [1540], val_loss: 26445382.0000\n", "Epoch [1560], val_loss: 26288784.0000\n", "Epoch [1580], val_loss: 26766956.0000\n", "Epoch [1600], val_loss: 26886406.0000\n", "Epoch [1620], val_loss: 26621722.0000\n", "Epoch [1640], val_loss: 27177836.0000\n", "Epoch [1660], val_loss: 26916280.0000\n", "Epoch [1680], val_loss: 26586590.0000\n", "Epoch [1700], val_loss: 27155032.0000\n", "Epoch [1720], val_loss: 27035532.0000\n", "Epoch [1740], val_loss: 27093464.0000\n", "Epoch [1760], val_loss: 26548204.0000\n", "Epoch [1780], val_loss: 27119994.0000\n", "Epoch [1800], val_loss: 26672474.0000\n", "Epoch [1820], val_loss: 26745840.0000\n", "Epoch [1840], val_loss: 27044198.0000\n", "Epoch [1860], val_loss: 27397692.0000\n", "Epoch [1880], val_loss: 26945804.0000\n", "Epoch [1900], val_loss: 26531612.0000\n", "Epoch [1920], val_loss: 26702884.0000\n", "Epoch [1940], val_loss: 27061996.0000\n", "Epoch [1960], val_loss: 26727144.0000\n", "Epoch [1980], val_loss: 26563086.0000\n", "Epoch [2000], val_loss: 26603478.0000\n", "Epoch [2020], val_loss: 26571240.0000\n", "Epoch [2040], val_loss: 26282350.0000\n", "Epoch [2060], val_loss: 27129160.0000\n", "Epoch [2080], val_loss: 26929620.0000\n", "Epoch [2100], val_loss: 26344294.0000\n", "Epoch [2120], val_loss: 27136092.0000\n", "Epoch [2140], val_loss: 26896984.0000\n", "Epoch [2160], val_loss: 27108664.0000\n", "Epoch [2180], val_loss: 26672748.0000\n", "Epoch [2200], val_loss: 26234068.0000\n", "Epoch [2220], val_loss: 26912500.0000\n", "Epoch [2240], val_loss: 26882076.0000\n", "Epoch [2260], val_loss: 26529786.0000\n", "Epoch [2280], val_loss: 26758946.0000\n", "Epoch [2300], val_loss: 26411978.0000\n", "Epoch [2320], val_loss: 27071890.0000\n", "Epoch [2340], val_loss: 26444848.0000\n", "Epoch [2360], val_loss: 26954912.0000\n", "Epoch [2380], val_loss: 27147724.0000\n", "Epoch [2400], val_loss: 26745032.0000\n", "Epoch [2420], val_loss: 26419416.0000\n", "Epoch [2440], val_loss: 27135066.0000\n", "Epoch [2460], val_loss: 26764714.0000\n", "Epoch [2480], val_loss: 26755796.0000\n", "Epoch [2500], val_loss: 26887486.0000\n", "Epoch [2520], val_loss: 26680276.0000\n", "Epoch [2540], val_loss: 26634662.0000\n", "Epoch [2560], val_loss: 26634020.0000\n", "Epoch [2580], val_loss: 26554988.0000\n", "Epoch [2600], val_loss: 26616368.0000\n", "Epoch [2620], val_loss: 26478438.0000\n", "Epoch [2640], val_loss: 26756898.0000\n", "Epoch [2660], val_loss: 26627632.0000\n", "Epoch [2680], val_loss: 26351406.0000\n", "Epoch [2700], val_loss: 26329702.0000\n", "Epoch [2720], val_loss: 26744876.0000\n", "Epoch [2740], val_loss: 26969288.0000\n", "Epoch [2760], val_loss: 26489110.0000\n", "Epoch [2780], val_loss: 27261618.0000\n", "Epoch [2800], val_loss: 26996398.0000\n", "Epoch [2820], val_loss: 26861702.0000\n", "Epoch [2840], val_loss: 27186640.0000\n", "Epoch [2860], val_loss: 26730208.0000\n", "Epoch [2880], val_loss: 27075394.0000\n", "Epoch [2900], val_loss: 26614206.0000\n", "Epoch [2920], val_loss: 26867942.0000\n", "Epoch [2940], val_loss: 27120706.0000\n", "Epoch [2960], val_loss: 26825690.0000\n", "Epoch [2980], val_loss: 26925824.0000\n", "Epoch [3000], val_loss: 26600464.0000\n", "Epoch [3020], val_loss: 26755762.0000\n", "Epoch [3040], val_loss: 26488342.0000\n", "Epoch [3060], val_loss: 26516016.0000\n", "Epoch [3080], val_loss: 27027920.0000\n", "Epoch [3100], val_loss: 26642066.0000\n", "Epoch [3120], val_loss: 26667848.0000\n", "Epoch [3140], val_loss: 26785268.0000\n", "Epoch [3160], val_loss: 27181234.0000\n", "Epoch [3180], val_loss: 26518136.0000\n", "Epoch [3200], val_loss: 26253446.0000\n", "Epoch [3220], val_loss: 26982790.0000\n", "Epoch [3240], val_loss: 26381206.0000\n", "Epoch [3260], val_loss: 26561930.0000\n", "Epoch [3280], val_loss: 26606592.0000\n", "Epoch [3300], val_loss: 27012628.0000\n", "Epoch [3320], val_loss: 26505314.0000\n", "Epoch [3340], val_loss: 26642092.0000\n", "Epoch [3360], val_loss: 27252606.0000\n", "Epoch [3380], val_loss: 27049362.0000\n", "Epoch [3400], val_loss: 26757820.0000\n", "Epoch [3420], val_loss: 26619580.0000\n", "Epoch [3440], val_loss: 26613870.0000\n", "Epoch [3460], val_loss: 26350186.0000\n", "Epoch [3480], val_loss: 26850700.0000\n", "Epoch [3500], val_loss: 27251336.0000\n", "Epoch [3520], val_loss: 26575248.0000\n", "Epoch [3540], val_loss: 26558688.0000\n", "Epoch [3560], val_loss: 27273112.0000\n", "Epoch [3580], val_loss: 27181804.0000\n", "Epoch [3600], val_loss: 26452132.0000\n", "Epoch [3620], val_loss: 26994734.0000\n", "Epoch [3640], val_loss: 26627580.0000\n", "Epoch [3660], val_loss: 26527682.0000\n", "Epoch [3680], val_loss: 27093386.0000\n", "Epoch [3700], val_loss: 26867944.0000\n", "Epoch [3720], val_loss: 26783034.0000\n", "Epoch [3740], val_loss: 26890374.0000\n", "Epoch [3760], val_loss: 27017160.0000\n", "Epoch [3780], val_loss: 26888936.0000\n", "Epoch [3800], val_loss: 27278572.0000\n", "Epoch [3820], val_loss: 26202986.0000\n", "Epoch [3840], val_loss: 27010700.0000\n", "Epoch [3860], val_loss: 26701594.0000\n", "Epoch [3880], val_loss: 26824904.0000\n", "Epoch [3900], val_loss: 26707276.0000\n", "Epoch [3920], val_loss: 26701598.0000\n", "Epoch [3940], val_loss: 26747008.0000\n", "Epoch [3960], val_loss: 26892958.0000\n", "Epoch [3980], val_loss: 26738258.0000\n", "Epoch [4000], val_loss: 26834688.0000\n", "Epoch [4020], val_loss: 26586010.0000\n", "Epoch [4040], val_loss: 26806032.0000\n", "Epoch [4060], val_loss: 26660128.0000\n", "Epoch [4080], val_loss: 27122452.0000\n", "Epoch [4100], val_loss: 26836270.0000\n", "Epoch [4120], val_loss: 26674178.0000\n", "Epoch [4140], val_loss: 26670470.0000\n", "Epoch [4160], val_loss: 26903170.0000\n", "Epoch [4180], val_loss: 26534572.0000\n", "Epoch [4200], val_loss: 26734256.0000\n", "Epoch [4220], val_loss: 26864876.0000\n", "Epoch [4240], val_loss: 26942468.0000\n", "Epoch [4260], val_loss: 26679578.0000\n", "Epoch [4280], val_loss: 26695148.0000\n", "Epoch [4300], val_loss: 27142716.0000\n", "Epoch [4320], val_loss: 26733976.0000\n", "Epoch [4340], val_loss: 26713410.0000\n", "Epoch [4360], val_loss: 27130160.0000\n", "Epoch [4380], val_loss: 26729594.0000\n", "Epoch [4400], val_loss: 26534368.0000\n", "Epoch [4420], val_loss: 27017304.0000\n", "Epoch [4440], val_loss: 27236984.0000\n", "Epoch [4460], val_loss: 26817734.0000\n", "Epoch [4480], val_loss: 26876844.0000\n", "Epoch [4500], val_loss: 27137938.0000\n", "Epoch [4520], val_loss: 26849744.0000\n", "Epoch [4540], val_loss: 26591818.0000\n", "Epoch [4560], val_loss: 27020378.0000\n", "Epoch [4580], val_loss: 26523734.0000\n", "Epoch [4600], val_loss: 26723698.0000\n", "Epoch [4620], val_loss: 26725400.0000\n", "Epoch [4640], val_loss: 27060128.0000\n", "Epoch [4660], val_loss: 26879058.0000\n", "Epoch [4680], val_loss: 27054600.0000\n", "Epoch [4700], val_loss: 26496048.0000\n", "Epoch [4720], val_loss: 26672928.0000\n", "Epoch [4740], val_loss: 26892242.0000\n", "Epoch [4760], val_loss: 26801792.0000\n", "Epoch [4780], val_loss: 26238176.0000\n", "Epoch [4800], val_loss: 26584148.0000\n", "Epoch [4820], val_loss: 26328446.0000\n", "Epoch [4840], val_loss: 27337212.0000\n", "Epoch [4860], val_loss: 26360772.0000\n", "Epoch [4880], val_loss: 26710032.0000\n", "Epoch [4900], val_loss: 27094972.0000\n", "Epoch [4920], val_loss: 27074948.0000\n", "Epoch [4940], val_loss: 26682544.0000\n", "Epoch [4960], val_loss: 26793382.0000\n", "Epoch [4980], val_loss: 26915032.0000\n", "Epoch [5000], val_loss: 26794802.0000\n", "Epoch [5020], val_loss: 26577690.0000\n", "Epoch [5040], val_loss: 27232946.0000\n", "Epoch [5060], val_loss: 26581526.0000\n", "Epoch [5080], val_loss: 26530904.0000\n", "Epoch [5100], val_loss: 26482086.0000\n", "Epoch [5120], val_loss: 27016610.0000\n", "Epoch [5140], val_loss: 27366628.0000\n", "Epoch [5160], val_loss: 26953490.0000\n", "Epoch [5180], val_loss: 27006998.0000\n", "Epoch [5200], val_loss: 26873860.0000\n", "Epoch [5220], val_loss: 26566654.0000\n", "Epoch [5240], val_loss: 26669930.0000\n", "Epoch [5260], val_loss: 26627432.0000\n", "Epoch [5280], val_loss: 27719394.0000\n", "Epoch [5300], val_loss: 26239176.0000\n", "Epoch [5320], val_loss: 26730592.0000\n", "Epoch [5340], val_loss: 26699430.0000\n", "Epoch [5360], val_loss: 26824688.0000\n", "Epoch [5380], val_loss: 26520880.0000\n", "Epoch [5400], val_loss: 26809814.0000\n", "Epoch [5420], val_loss: 26643924.0000\n", "Epoch [5440], val_loss: 26709508.0000\n", "Epoch [5460], val_loss: 26584424.0000\n", "Epoch [5480], val_loss: 27342398.0000\n", "Epoch [5500], val_loss: 26490656.0000\n", "Epoch [5520], val_loss: 26403960.0000\n", "Epoch [5540], val_loss: 26595448.0000\n", "Epoch [5560], val_loss: 27120262.0000\n", "Epoch [5580], val_loss: 26901004.0000\n", "Epoch [5600], val_loss: 26644840.0000\n", "Epoch [5620], val_loss: 26625464.0000\n", "Epoch [5640], val_loss: 26949516.0000\n", "Epoch [5660], val_loss: 26568442.0000\n", "Epoch [5680], val_loss: 26733256.0000\n", "Epoch [5700], val_loss: 26446714.0000\n", "Epoch [5720], val_loss: 27424638.0000\n", "Epoch [5740], val_loss: 26698826.0000\n", "Epoch [5760], val_loss: 26527906.0000\n", "Epoch [5780], val_loss: 27401962.0000\n", "Epoch [5800], val_loss: 26556984.0000\n", "Epoch [5820], val_loss: 26470834.0000\n", "Epoch [5840], val_loss: 26658182.0000\n", "Epoch [5860], val_loss: 26813968.0000\n", "Epoch [5880], val_loss: 27036264.0000\n", "Epoch [5900], val_loss: 27298012.0000\n", "Epoch [5920], val_loss: 27360670.0000\n", "Epoch [5940], val_loss: 27602944.0000\n", "Epoch [5960], val_loss: 26912538.0000\n", "Epoch [5980], val_loss: 27016030.0000\n", "Epoch [6000], val_loss: 26597432.0000\n", "Epoch [6020], val_loss: 26496410.0000\n", "Epoch [6040], val_loss: 26413948.0000\n", "Epoch [6060], val_loss: 27186672.0000\n", "Epoch [6080], val_loss: 26529776.0000\n", "Epoch [6100], val_loss: 26588152.0000\n", "Epoch [6120], val_loss: 27242990.0000\n", "Epoch [6140], val_loss: 26578436.0000\n", "Epoch [6160], val_loss: 26884080.0000\n", "Epoch [6180], val_loss: 26815814.0000\n", "Epoch [6200], val_loss: 26640792.0000\n", "Epoch [6220], val_loss: 26704054.0000\n", "Epoch [6240], val_loss: 27531858.0000\n", "Epoch [6260], val_loss: 26877836.0000\n", "Epoch [6280], val_loss: 26907804.0000\n", "Epoch [6300], val_loss: 26639234.0000\n", "Epoch [6320], val_loss: 26465414.0000\n", "Epoch [6340], val_loss: 27366950.0000\n", "Epoch [6360], val_loss: 27234076.0000\n", "Epoch [6380], val_loss: 26512012.0000\n", "Epoch [6400], val_loss: 26742990.0000\n", "Epoch [6420], val_loss: 26810502.0000\n", "Epoch [6440], val_loss: 27391410.0000\n", "Epoch [6460], val_loss: 26465540.0000\n", "Epoch [6480], val_loss: 26928558.0000\n", "Epoch [6500], val_loss: 26830928.0000\n", "Epoch [6520], val_loss: 26828470.0000\n", "Epoch [6540], val_loss: 26659078.0000\n", "Epoch [6560], val_loss: 26813270.0000\n", "Epoch [6580], val_loss: 26532560.0000\n", "Epoch [6600], val_loss: 27070310.0000\n", "Epoch [6620], val_loss: 27043630.0000\n", "Epoch [6640], val_loss: 26868988.0000\n", "Epoch [6660], val_loss: 26810554.0000\n", "Epoch [6680], val_loss: 26989184.0000\n", "Epoch [6700], val_loss: 27009380.0000\n", "Epoch [6720], val_loss: 26684612.0000\n", "Epoch [6740], val_loss: 26652238.0000\n", "Epoch [6760], val_loss: 27366978.0000\n", "Epoch [6780], val_loss: 26703798.0000\n", "Epoch [6800], val_loss: 26301814.0000\n", "Epoch [6820], val_loss: 26534066.0000\n", "Epoch [6840], val_loss: 26699594.0000\n", "Epoch [6860], val_loss: 27092482.0000\n", "Epoch [6880], val_loss: 26884104.0000\n", "Epoch [6900], val_loss: 26926348.0000\n", "Epoch [6920], val_loss: 26928012.0000\n", "Epoch [6940], val_loss: 26744056.0000\n", "Epoch [6960], val_loss: 26917726.0000\n", "Epoch [6980], val_loss: 26609290.0000\n", "Epoch [7000], val_loss: 27304262.0000\n", "Epoch [7020], val_loss: 26712178.0000\n", "Epoch [7040], val_loss: 26495484.0000\n", "Epoch [7060], val_loss: 26846470.0000\n", "Epoch [7080], val_loss: 26781522.0000\n", "Epoch [7100], val_loss: 26684870.0000\n", "Epoch [7120], val_loss: 26625908.0000\n", "Epoch [7140], val_loss: 26669390.0000\n", "Epoch [7160], val_loss: 27570038.0000\n", "Epoch [7180], val_loss: 27450736.0000\n", "Epoch [7200], val_loss: 26556048.0000\n", "Epoch [7220], val_loss: 26407316.0000\n", "Epoch [7240], val_loss: 26470270.0000\n", "Epoch [7260], val_loss: 27134890.0000\n", "Epoch [7280], val_loss: 26813732.0000\n", "Epoch [7300], val_loss: 26726668.0000\n", "Epoch [7320], val_loss: 26704558.0000\n", "Epoch [7340], val_loss: 26902860.0000\n", "Epoch [7360], val_loss: 26659964.0000\n", "Epoch [7380], val_loss: 26199952.0000\n", "Epoch [7400], val_loss: 26461926.0000\n", "Epoch [7420], val_loss: 26526322.0000\n", "Epoch [7440], val_loss: 26489390.0000\n", "Epoch [7460], val_loss: 26430412.0000\n", "Epoch [7480], val_loss: 26762028.0000\n", "Epoch [7500], val_loss: 26762830.0000\n", "Epoch [7520], val_loss: 27196898.0000\n", "Epoch [7540], val_loss: 26763588.0000\n", "Epoch [7560], val_loss: 27041662.0000\n", "Epoch [7580], val_loss: 26478146.0000\n", "Epoch [7600], val_loss: 27146810.0000\n", "Epoch [7620], val_loss: 26325586.0000\n", "Epoch [7640], val_loss: 26735070.0000\n", "Epoch [7660], val_loss: 26607042.0000\n", "Epoch [7680], val_loss: 26659762.0000\n", "Epoch [7700], val_loss: 27159946.0000\n", "Epoch [7720], val_loss: 26704494.0000\n", "Epoch [7740], val_loss: 26449608.0000\n", "Epoch [7760], val_loss: 26632102.0000\n", "Epoch [7780], val_loss: 26619360.0000\n", "Epoch [7800], val_loss: 26777782.0000\n", "Epoch [7820], val_loss: 26545498.0000\n", "Epoch [7840], val_loss: 26973582.0000\n", "Epoch [7860], val_loss: 27204072.0000\n", "Epoch [7880], val_loss: 26738606.0000\n", "Epoch [7900], val_loss: 27153638.0000\n", "Epoch [7920], val_loss: 26620946.0000\n", "Epoch [7940], val_loss: 26757490.0000\n", "Epoch [7960], val_loss: 26863116.0000\n", "Epoch [7980], val_loss: 26875934.0000\n", "Epoch [8000], val_loss: 26843992.0000\n", "Epoch [8020], val_loss: 26796446.0000\n", "Epoch [8040], val_loss: 26541758.0000\n", "Epoch [8060], val_loss: 26541260.0000\n", "Epoch [8080], val_loss: 26679244.0000\n", "Epoch [8100], val_loss: 27018488.0000\n", "Epoch [8120], val_loss: 26393474.0000\n", "Epoch [8140], val_loss: 26673686.0000\n", "Epoch [8160], val_loss: 26842664.0000\n", "Epoch [8180], val_loss: 26319426.0000\n", "Epoch [8200], val_loss: 26796460.0000\n", "Epoch [8220], val_loss: 27382508.0000\n", "Epoch [8240], val_loss: 27314942.0000\n", "Epoch [8260], val_loss: 27155296.0000\n", "Epoch [8280], val_loss: 26962180.0000\n", "Epoch [8300], val_loss: 26873342.0000\n", "Epoch [8320], val_loss: 26428406.0000\n", "Epoch [8340], val_loss: 27035716.0000\n", "Epoch [8360], val_loss: 26773630.0000\n", "Epoch [8380], val_loss: 26882604.0000\n", "Epoch [8400], val_loss: 26671458.0000\n", "Epoch [8420], val_loss: 26513846.0000\n", "Epoch [8440], val_loss: 26827942.0000\n", "Epoch [8460], val_loss: 26664438.0000\n", "Epoch [8480], val_loss: 26552214.0000\n", "Epoch [8500], val_loss: 26856976.0000\n", "Epoch [8520], val_loss: 26736962.0000\n", "Epoch [8540], val_loss: 27136554.0000\n", "Epoch [8560], val_loss: 26921782.0000\n", "Epoch [8580], val_loss: 26548398.0000\n", "Epoch [8600], val_loss: 26590122.0000\n", "Epoch [8620], val_loss: 26561870.0000\n", "Epoch [8640], val_loss: 27007810.0000\n", "Epoch [8660], val_loss: 26854604.0000\n", "Epoch [8680], val_loss: 27652830.0000\n", "Epoch [8700], val_loss: 26772940.0000\n", "Epoch [8720], val_loss: 26399682.0000\n", "Epoch [8740], val_loss: 26612988.0000\n", "Epoch [8760], val_loss: 26685208.0000\n", "Epoch [8780], val_loss: 27198586.0000\n", "Epoch [8800], val_loss: 26831774.0000\n", "Epoch [8820], val_loss: 27263844.0000\n", "Epoch [8840], val_loss: 26916644.0000\n", "Epoch [8860], val_loss: 27076260.0000\n", "Epoch [8880], val_loss: 26511978.0000\n", "Epoch [8900], val_loss: 26641776.0000\n", "Epoch [8920], val_loss: 26755446.0000\n", "Epoch [8940], val_loss: 26881964.0000\n", "Epoch [8960], val_loss: 26480564.0000\n", "Epoch [8980], val_loss: 26601388.0000\n", "Epoch [9000], val_loss: 26733962.0000\n", "Epoch [9020], val_loss: 26529576.0000\n", "Epoch [9040], val_loss: 26447392.0000\n", "Epoch [9060], val_loss: 26511976.0000\n", "Epoch [9080], val_loss: 26555580.0000\n", "Epoch [9100], val_loss: 26788030.0000\n", "Epoch [9120], val_loss: 26457144.0000\n", "Epoch [9140], val_loss: 26993498.0000\n", "Epoch [9160], val_loss: 26528072.0000\n", "Epoch [9180], val_loss: 26885456.0000\n", "Epoch [9200], val_loss: 26189096.0000\n", "Epoch [9220], val_loss: 26852674.0000\n", "Epoch [9240], val_loss: 26878120.0000\n", "Epoch [9260], val_loss: 27091956.0000\n", "Epoch [9280], val_loss: 26613836.0000\n", "Epoch [9300], val_loss: 27038530.0000\n", "Epoch [9320], val_loss: 27328508.0000\n", "Epoch [9340], val_loss: 27036018.0000\n", "Epoch [9360], val_loss: 27080014.0000\n", "Epoch [9380], val_loss: 27299248.0000\n", "Epoch [9400], val_loss: 26839076.0000\n", "Epoch [9420], val_loss: 26732546.0000\n", "Epoch [9440], val_loss: 26742442.0000\n", "Epoch [9460], val_loss: 26761506.0000\n", "Epoch [9480], val_loss: 26658178.0000\n", "Epoch [9500], val_loss: 26613248.0000\n", "Epoch [9520], val_loss: 27144180.0000\n", "Epoch [9540], val_loss: 26567960.0000\n", "Epoch [9560], val_loss: 26477742.0000\n", "Epoch [9580], val_loss: 26379192.0000\n", "Epoch [9600], val_loss: 26509548.0000\n", "Epoch [9620], val_loss: 26795278.0000\n", "Epoch [9640], val_loss: 26781076.0000\n", "Epoch [9660], val_loss: 27517908.0000\n", "Epoch [9680], val_loss: 26466418.0000\n", "Epoch [9700], val_loss: 27087420.0000\n", "Epoch [9720], val_loss: 26718168.0000\n", "Epoch [9740], val_loss: 26835790.0000\n", "Epoch [9760], val_loss: 26960112.0000\n", "Epoch [9780], val_loss: 26846336.0000\n", "Epoch [9800], val_loss: 27091680.0000\n", "Epoch [9820], val_loss: 26386688.0000\n", "Epoch [9840], val_loss: 26492920.0000\n", "Epoch [9860], val_loss: 27024494.0000\n", "Epoch [9880], val_loss: 26905506.0000\n", "Epoch [9900], val_loss: 26344502.0000\n", "Epoch [9920], val_loss: 26836516.0000\n", "Epoch [9940], val_loss: 26767882.0000\n", "Epoch [9960], val_loss: 26445790.0000\n", "Epoch [9980], val_loss: 26758378.0000\n", "Epoch [10000], val_loss: 26793100.0000\n"]}], "source": ["epochs = 10000\n", "lr = 1e-5\n", "history5 = fit(epochs, lr, model, train_loader, val_loader)"]}, {"cell_type": "markdown", "metadata": {"id": "Sy9078JCRB9G"}, "source": ["**Q: What is the final validation loss of your model?**"]}, {"cell_type": "code", "execution_count": 208, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1227, "status": "ok", "timestamp": 1607756566230, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "yiCwL6b5RB9G", "outputId": "ce0aac7a-9a1e-4216-b791-ffab177731a3"}, "outputs": [{"data": {"text/plain": ["{'val_loss': 26793100.0}"]}, "execution_count": 208, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["val_loss = evaluate(model, val_loader)\n", "val_loss"]}, {"cell_type": "markdown", "metadata": {"id": "v74eQTJ2RB9G"}, "source": ["Let's log the final validation loss to Jovian and commit the notebook"]}, {"cell_type": "code", "execution_count": 209, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1042, "status": "ok", "timestamp": 1607756570273, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "VeSkSGZmRB9H", "outputId": "494608bc-b2d3-4286-8c66-15be8edd5f0a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[jovian] Metrics logged.\u001b[0m\n"]}], "source": ["jovian.log_metrics(val_loss=val_loss)"]}, {"cell_type": "code", "execution_count": 210, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 103}, "executionInfo": {"elapsed": 4739, "status": "ok", "timestamp": 1607756575587, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "BkYEhBlaRB9H", "outputId": "c55857bb-0868-4595-f60d-130f319d44d0"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[jovian] Detected Colab notebook...\u001b[0m\n", "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n", "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n", "[jovian] Committed successfully! https://jovian.ai/anurag3301/02-insurance-linear-regression\u001b[0m\n"]}, {"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'https://jovian.ai/anurag3301/02-insurance-linear-regression'"]}, "execution_count": 210, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["jovian.commit(project=project_name, environment=None)"]}, {"cell_type": "markdown", "metadata": {"id": "0xso1nZWRB9H"}, "source": ["Now scroll back up, re-initialize the model, and try different set of values for batch size, number of epochs, learning rate etc. Commit each experiment and use the \"Compare\" and \"View Diff\" options on Jovian to compare the different results."]}, {"cell_type": "markdown", "metadata": {"id": "XJlPJ-lfRB9H"}, "source": ["## Step 5: Make predictions using the trained model\n", "\n", "**Q: Complete the following function definition to make predictions on a single input**"]}, {"cell_type": "markdown", "metadata": {"id": "AuHgHmBHhQmW"}, "source": []}, {"cell_type": "code", "execution_count": 211, "metadata": {"executionInfo": {"elapsed": 1255, "status": "ok", "timestamp": 1607756579230, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "kmSmUoZdRB9I"}, "outputs": [], "source": ["def predict_single(input, target, model):\n", "    inputs = input.unsqueeze(0)\n", "    predictions = model(inputs)                # fill this\n", "    prediction = predictions[0].detach()\n", "    print(\"Input:\", input)\n", "    print(\"Target:\", target)\n", "    print(\"Prediction:\", prediction)"]}, {"cell_type": "code", "execution_count": 212, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1464, "status": "ok", "timestamp": 1607756582495, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "fwSdeh-vRB9I", "outputId": "e2f40d84-8c6b-4315-f406-2f7dbe646814"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Input: tensor([18.0000,  0.0000, 30.0080,  3.0000,  1.0000,  2.0000])\n", "Target: tensor([21321.4375])\n", "Prediction: tensor([31445.3047])\n"]}], "source": ["input, target = val_ds[0]\n", "predict_single(input, target, model)"]}, {"cell_type": "code", "execution_count": 213, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1447, "status": "ok", "timestamp": 1607756588197, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "qA9AJnOdRB9I", "outputId": "c1266864-14a6-41cc-db8e-909bb827e2d4"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Input: tensor([47.0000,  0.0000, 25.9600,  1.0000,  0.0000,  3.0000])\n", "Target: tensor([9991.4150])\n", "Prediction: tensor([9323.8350])\n"]}], "source": ["input, target = val_ds[10]\n", "predict_single(input, target, model)"]}, {"cell_type": "code", "execution_count": 214, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1337, "status": "ok", "timestamp": 1607756590681, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "XS3-mgKXRB9I", "outputId": "99fb0420-c9b5-4bfa-d11f-0157a640cab6"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Input: tensor([53.0000,  0.0000, 39.4900,  2.0000,  0.0000,  3.0000])\n", "Target: tensor([13061.3750])\n", "Prediction: tensor([15590.8486])\n"]}], "source": ["input, target = val_ds[23]\n", "predict_single(input, target, model)"]}, {"cell_type": "markdown", "metadata": {"id": "YsYtFfRVRB9J"}, "source": ["Are you happy with your model's predictions? Try to improve them further."]}, {"cell_type": "markdown", "metadata": {"id": "AFs0ga6gRB9J"}, "source": ["## (Optional) Step 6: Try another dataset & blog about it\n", "\n", "While this last step is optional for the submission of your assignment, we highly recommend that you do it. Try to replicate this notebook for a different linear regression or logistic regression problem. This will help solidify your understanding, and give you a chance to differentiate the generic patterns in machine learning from problem-specific details.You can use one of these starer notebooks (just change the dataset):\n", "\n", "- Linear regression (minimal): https://jovian.ai/aakashns/housing-linear-minimal\n", "- Logistic regression (minimal): https://jovian.ai/aakashns/mnist-logistic-minimal\n", "\n", "Here are some sources to find good datasets:\n", "\n", "- https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/\n", "- https://www.kaggle.com/rtatman/datasets-for-regression-analysis\n", "- https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table\n", "- https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\n", "- https://archive.ics.uci.edu/ml/datasets/wine+quality\n", "- https://pytorch.org/docs/stable/torchvision/datasets.html\n", "\n", "We also recommend that you write a blog about your approach to the problem. Here is a suggested structure for your post (feel free to experiment with it):\n", "\n", "- Interesting title & subtitle\n", "- Overview of what the blog covers (which dataset, linear regression or logistic regression, intro to PyTorch)\n", "- Downloading & exploring the data\n", "- Preparing the data for training\n", "- Creating a model using PyTorch\n", "- Training the model to fit the data\n", "- Your thoughts on how to experiment with different hyperparmeters to reduce loss\n", "- Making predictions using the model\n", "\n", "As with the previous assignment, you can [embed Juptyer notebook cells & outputs from Jovian](https://medium.com/jovianml/share-and-embed-jupyter-notebooks-online-with-jovian-ml-df709a03064e) into your blog. \n", "\n", "Don't forget to share your work on the forum: https://jovian.ai/forum/t/linear-regression-and-logistic-regression-notebooks-and-blog-posts/14039"]}, {"cell_type": "code", "execution_count": 184, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 171}, "executionInfo": {"elapsed": 8095, "status": "ok", "timestamp": 1607754115014, "user": {"displayName": "Anurag Kumar", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjgR0FduwEOvJzDZWcVdQ_nLJ9BsCAhRi2VFbJ7nA=s64", "userId": "10912555925963999343"}, "user_tz": -330}, "id": "43VcrIeLRB9J", "outputId": "e89f1d60-0f57-4a58-e8de-f10ebaf5206f"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[jovian] Detected Colab notebook...\u001b[0m\n", "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n", "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n", "[jovian] Committed successfully! https://jovian.ai/anurag3301/02-insurance-linear-regression\u001b[0m\n", "[jovian] Detected Colab notebook...\u001b[0m\n", "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n", "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n", "[jovian] Committed successfully! https://jovian.ai/anurag3301/02-insurance-linear-regression\u001b[0m\n"]}, {"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'https://jovian.ai/anurag3301/02-insurance-linear-regression'"]}, "execution_count": 184, "metadata": {"tags": []}, "output_type": "execute_result"}], "source": ["\n", "jovian.commit(project=project_name, environment=None) # try again, kaggle fails sometimes"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "QvviNslhRB9K"}, "outputs": [], "source": []}], "metadata": {"colab": {"name": "02-insurance-linear.ipynb", "provenance": []}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 0}